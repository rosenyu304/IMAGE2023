---
title: "Amortized Bayesian Full Waveform Inversion and Experimental Design with Normalizing Flows"
author:
  - name: Rafael Orozco
    affiliations:
      name: Georgia Institute of Technology
  - name: Mathias Louboutin
    affiliations:
      name: Georgia Institute of Technology
  - name: Felix J. Herrmann
    affiliations:
      name: Georgia Institute of Technology
description: | 
  empty
bibliography: abstract.bib
abstract: |
  empty
---

# Image 2023 abstract

## OBJECTIVES AND SCOPE (638/600 characters)
Probabilistic approaches to Full-Waveform Inversion (FWI), such as Bayesian ones, traditionally require expensive computations involving multiple wave-equation solves. To reduce this computational burden at test time, we propose to amortize the computational cost with offline training. After training, our method aims to efficiently generate probabilistic FWI solutions with uncertainty. This aim is achieved by exploiting the ability of deep networks (i.e. Normalizing Flows) to learn distributions, in our case, the Bayesian posterior. The posterior uncertainty is used during training to find the receiver optimal experimental design (OED).

## METHODS, PROCEDURES, PROCESS (1392/1500 characters)
Normalizing flows (NFs) are generative networks that can learn to sample from conditional distributions, i.e. our desired Bayesian posterior p(x\|y) where x are earth models and y is seismic data. To train NFs, we require training pairs of these earth models and seismic data. We use pairs from the open source dataset OPENFWI. The earth models are 64x64 size and the seismic data is simulated with 5 source experiments with 15Hz frequency and 20dB noise. We use the CurveFault_B dataset from OPENFWI that contains 55k pairs. We use a 90%/5%/5% split for training, validation, and testing.

To learn to sample from the conditional distribution, the network learns to transform the earth models to Gaussian normal noise while conditioned on the acoustic data. By construction the network is invertible, thus after training we resample Gaussian normal noise and pass it through the inverse network to generate samples from the posterior. The training process is illustrated schematically in Figure 1.

The NFs training objective has been shown to be equivalent to maximizing the information gain (as defined by Bayesian OED) of the conditioned data. Thus, we propose to jointly optimize a mask M that occludes receivers of the seismic data (M.\*y) during NF training. Our optimization jointly optimizes for NF parameters and optimal mask M values in a single objective (Equation 1 in Figure 1).

## RESULTS, OBSERVATIONS, CONCLUSIONS  (1617/1500 characters)
After training, our method generates posterior samples at the cost of one neural network pass (10ms on our GPU). Seen in Figure 2, the posterior samples are realistic earth models that could plausibly match the seismic data. The variations between the models are due to the FWI problem being ill-posed and because of noise in the indirect observations at the subsurface. To study these variations, we take the sample variance as our uncertainty quantification. We note that the uncertainty is concentrated on vertical and deeper structures. Both these structures are difficult to image with FWI leading to the pleasing result that our uncertainty correlates with error made making our uncertainty useful and interpretable. 

To make a single high-quality solution, we take the mean of the posterior samples. The posterior mean shows high-quality metrics that surpass previous benchmarks on the OPENFWI dataset while our method also produces Bayesian uncertainty and the benchmark methods do not.

The OED found by our method (filled in circles in Figure 2 represent optimal receiver locations) matches prior expectations that the most important information is contained in the short offsets. This result was independently discovered by the Bayesian optimization routine.

Our method is amortized since the previously mentioned computational debt associated with probabilistic FWI is paid by the forward modeling needed to compute training pairs (x,y) plus the cost of training the network. After both these costs are incurred offline, our method can efficiently sample from the Bayesian posterior for many unseen seismic datasets. 

## SIGNIFICANCE/NOVELTY (378/600 characters)
To our knowledge, this is the first demonstration of conditional normalizing flows on amortized FWI with uncertainty quantification. We also showed a practical application of the uncertainty towards experimental design. While the open source dataset has small models, NFs are memory efficient due to their intrinsic invertibility so are set to scale to realistic sized problems.