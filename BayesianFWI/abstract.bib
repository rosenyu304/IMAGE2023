@inproceedings{muller2021learning,
  title={Learning robust models using the principle of independent causal mechanisms},
  author={M{\"u}ller, Jens and Schmier, Robert and Ardizzone, Lynton and Rother, Carsten and K{\"o}the, Ullrich},
  booktitle={DAGM German Conference on Pattern Recognition},
  pages={79--110},
  year={2021},
  organization={Springer}
}

@inproceedings{deans2002maximally,
  title={Maximally informative statistics for localization and mapping},
  author={Deans, Matthew C},
  booktitle={Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No. 02CH37292)},
  volume={2},
  pages={1824--1829},
  year={2002},
  organization={IEEE}
}

@article{radev2020bayesflow,
  title={BayesFlow: Learning complex stochastic models with invertible neural networks},
  author={Radev, Stefan T and Mertens, Ulf K and Voss, Andreas and Ardizzone, Lynton and K{\"o}the, Ullrich},
  journal={IEEE transactions on neural networks and learning systems},
  year={2020},
  publisher={IEEE}
}

@article{bloem2020probabilistic,
  title={Probabilistic Symmetries and Invariant Neural Networks.},
  author={Bloem-Reddy, Benjamin and Teh, Yee Whye},
  journal={J. Mach. Learn. Res.},
  volume={21},
  pages={90--1},
  year={2020}
}

@article{devito-api,
  author = {Louboutin, M. and Lange, M. and Luporini, F. and Kukreja, N. and Witte, P. A. and Herrmann, F. J. and Velesko, P. and Gorman, G. J.},
  title = {Devito (v3.1.0): an embedded domain-specific language for finite differences and geophysical exploration},
  journal = {{Geoscientific Model Development}},
  volume = {12},
  year = {2019},
  number = {3},
  pages = {1165--1187},
  url = {https://www.geosci-model-dev.net/12/1165/2019/},
  doi = {10.5194/gmd-12-1165-2019}
}

@article{devito-compiler,
author = {Luporini, Fabio and Louboutin, Mathias and Lange, Michael and Kukreja, Navjot and Witte, Philipp and H\"{u}ckelheim, Jan and Yount, Charles and Kelly, Paul H. J. and Herrmann, Felix J. and Gorman, Gerard J.},
title = {Architecture and Performance of Devito, a System for Automated Stencil Computation},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/3374916},
doi = {10.1145/3374916},
abstract = {Stencil computations are a key part of many high-performance computing applications, such as image processing, convolutional neural networks, and finite-difference solvers for partial differential equations. Devito is a framework capable of generating highly optimized code given symbolic equations expressed in Python, specialized in, but not limited to, affine (stencil) codes. The lowering process—from mathematical equations down to C++ code—is performed by the Devito compiler through a series of intermediate representations. Several performance optimizations are introduced, including advanced common sub-expressions elimination, tiling, and parallelization. Some of these are obtained through well-established stencil optimizers, integrated in the backend of the Devito compiler. The architecture of the Devito compiler, as well as the performance optimizations that are applied when generating code, are presented. The effectiveness of such performance optimizations is demonstrated using operators drawn from seismic imaging applications.},
journal = {ACM Trans. Math. Softw.},
month = apr,
articleno = {6},
numpages = {28},
keywords = {symbolic processing, stencil, structured grid, compiler, Finite-difference method, performance optimization, domain-specific language}
}

@article{witte2018alf,
	title = {A large-scale framework for symbolic implementations of seismic inversion algorithms in Julia},
	journal = {Geophysics},
	volume = {84},
	number = {3},
	year = {2019},
	note = {(Geophysics)},
	month = {03},
	pages = {F57-F71},
	keywords = {FWI, inversion, LSRTM, Modeling, software},
	doi = {10.1190/geo2018-0174.1},
	url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2019/witte2018alf/witte2018alf.pdf},
	author = {Philipp A. Witte and Mathias Louboutin and Navjot Kukreja and Fabio Luporini and Michael Lange and Gerard J. Gorman and Felix J. Herrmann}
}


@article{virieux,
  author	= {J. Virieux and S. Operto},
  doi		= {10.1190/1.3238367},
  eprint	= {http://library.seg.org/doi/pdf/10.1190/1.3238367},
  journal	= {GEOPHYSICS},
  number	= {5},
  pages		= {WCC1-WCC26},
  title		= {An overview of full-waveform inversion in exploration
		  geophysics},
  url		= {http://library.seg.org/doi/abs/10.1190/1.3238367},
  volume	= {74},
  year		= {2009},
  bdsk-url-1	= {http://library.seg.org/doi/abs/10.1190/1.3238367},
  bdsk-url-2	= {http://dx.doi.org/10.1190/1.3238367}
}

@article{haber10tremp,
  abstract	= {Often, parameter estimation problems of
		  parameter-dependent PDEs involve multiple right-hand sides.
		  The computational cost and memory requirements of such
		  problems increase linearly with the number of right-hand
		  sides. For many applications this is the main bottleneck of
		  the computation. In this paper we show that problems with
		  multiple right-hand sides can be reformulated as stochastic
		  programming problems by combining the right-hand sides into
		  a few {\quotedblbase}simultaneous{\textquotedblright}
		  sources. This effectively reduces the cost of the forward
		  problem and results in problems that are much cheaper to
		  solve. We discuss two solution methodologies: namely sample
		  average approximation and stochastic approximation. To
		  illustrate the effectiveness of our approach we present two
		  model problems, direct current resistivity and seismic
		  tomography.},
  author	= {Eldad Haber and Matthias Chung and Felix J. Herrmann},
  journal	= {SIAM Journal on Optimization},
  keywords	= {FWI, Optimization, SLIM},
  month		= {7},
  number	= {3},
  publisher	= {UBC-Earth and Ocean Sciences Department},
  title		= {An effective method for parameter estimation with {PDE}
		  constraints with multiple right hand sides},
  url		= {http://dx.doi.org/10.1137/11081126X},
  volume	= {22},
  year		= {2012},
  bdsk-url-1	= {http://dx.doi.org/10.1137/11081126X}
}

@article{tarantola,
  author	= {Tarantola, Albert},
  title		= {Inversion of seismic reflection data in the acoustic
		  approximation},
  journal	= {GEOPHYSICS},
  volume	= {49},
  number	= {8},
  pages		= {1259},
  year		= {1984},
  doi		= {10.1190/1.1441754},
  url		= { + http://dx.doi.org/10.1190/1.1441754},
  eprint	= {/gsw/content_public/journal/geophysics/49/8/10.1190_1.1441754/5/1259.pdf}
}

@Book{		  lionsjl1971,
  author	= {Lions, J. L.},
  edition	= {1st},
  isbn		= {978-3-642-65026-0},
  publisher	= {Springer-Verlag Berlin Heidelberg},
  title		= {Optimal control of systems governed by partial
		  differential equations},
  year		= {1971}
}


@Article{	  plessixasfwi,
  author	= {Plessix, R.-E.},
  doi		= {10.1111/j.1365-246X.2006.02978.x},
  issn		= {1365-246X},
  journal	= {Geophysical Journal International},
  keywords	= {adjoint state, gradient, migration, tomography},
  number	= {2},
  pages		= {495--503},
  publisher	= {Blackwell Publishing Ltd},
  title		= {A review of the adjoint-state method for computing the
		  gradient of a functional with geophysical applications},
  url		= {http://dx.doi.org/10.1111/j.1365-246X.2006.02978.x},
  volume	= {167},
  year		= {2006},
  bdsk-url-1	= {http://dx.doi.org/10.1111/j.1365-246X.2006.02978.x}
}

@article {louboutin2017fwi,
	title = {Full-Waveform Inversion - Part 1: forward modeling},
	journal = {The Leading Edge},
	volume = {36},
	number = {12},
	year = {2017},
	note = {(The Leading Edge)},
	month = {12},
	pages = {1033-1036},
	abstract = {Since its re-introduction by Pratt (1999), full-waveform inversion (FWI) has gained a lot of attention in geophysical exploration because of its ability to build high resolution velocity models more or less automatically in areas of complex geology. While there is an extensive and growing literature on the topic, publications focus mostly on technical aspects, making this topic inaccessible for a broader audience due to the lack of simple introductory resources for newcomers to geophysics. We will accomplish this by providing a hands-on walkthrough of FWI using Devito (Lange et al. 2016), a system based on domain-specific languages that automatically generates code for time-domainfinite-differences.},
	keywords = {devito, finite-differences, FWI, Modeling, tutorial},
	doi = {10.1190/tle36121033.1},
	url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2017/louboutin2017fwi/louboutin2017fwi.html},
	author = {Mathias Louboutin and Philipp A. Witte and Michael Lange and Navjot Kukreja and Fabio Luporini and Gerard Gorman and Felix J. Herrmann}
}

@article {louboutin2017fwip2,
	title = {Full-Waveform Inversion - Part 2: adjoint modeling},
	journal = {The Leading Edge},
	volume = {37},
	number = {1},
	year = {2018},
	note = {(The Leading Edge)},
	month = {1},
	pages = {69-72},
	abstract = {This tutorial is the second part of a three part tutorial series on full-waveform inversion (FWI), in which we provide a step by step walk through of setting up forward and adjoint wave equation solvers and an optimization framework for inversion. In part 1 (Louboutin et al., 2017), we demonstrated how to discretize the acoustic wave equation and how to set up a basic forward modeling scheme using Devito, a domain-specific language (DSL) in Python for automated finite-difference (FD) computations (Lange et al., 2016). Devito allows us to define wave equations as symbolic Python expressions (Meurer et al., 2017), from which optimized FD stencil code is automatically generated at run time. In part 1, we show how we can use Devito to set up and solve acoustic wave equations with (impulsive) seismic sources and sample wavefields at the receiver locations to model shot records.},
	keywords = {acoustic, devito, finite-difference, FWI, tutorial},
	doi = {10.1190/tle37010069.1},
	url = {https://slim.gatech.edu/Publications/Public/Journals/TheLeadingEdge/2018/louboutin2017fwip2/louboutin2017fwip2.html},
	author = {Mathias Louboutin and Philipp A. Witte and Michael Lange and Navjot Kukreja and Fabio Luporini and Gerard Gorman and Felix J. Herrmann}
}

@Conference{	  louboutin2015segtcs,
  author = {Mathias Louboutin and Felix J. Herrmann},
  booktitle	= {SEG Technical Program Expanded Abstracts},
  doi		= {10.1190/segam2015-5924937.1},
  keywords	= {acoustic, Full-waveform inversion, inversion, SEG,
		  Stochastic optimization, Subsampling, Time-domain},
  month		= {10},
  note		= {(SEG, New Orleans)},
  pages		= {5153-5157},
  presentation	= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2015/louboutin2015SEGtcs/louboutin2015SEGtcs_poster.pdf},
  title		= {Time compressively sampled full-waveform inversion with
		  stochastic optimization},
  url		= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2015/louboutin2015SEGtcs/louboutin2015SEGtcs.html},
  year		= {2015},
  bdsk-url-1	= {https://www.slim.eos.ubc.ca/Publications/Public/Conferences/SEG/2015/louboutin2015SEGtcs/louboutin2015SEGtcs.html},
  bdsk-url-2	= {http://dx.doi.org/10.1190/segam2015-5924937.1}
}

@article{   witte2018cls,
	title = {Compressive least-squares migration with on-the-fly Fourier transforms},
	journal = {Geophysics},
	volume = {84},
	number = {5},
	year = {2019},
	note = {(Geophysics)},
	month = {08},
	pages = {R655-R672},
	abstract = {Least-squares reverse-time migration is a powerful approach for true amplitude seismic imaging of complex geological structures, but the successful application of this method is currently hindered by its enormous computational cost, as well as high memory requirements for computing the gradient of the objective function. We tackle these problems by introducing an algorithm for low-cost sparsity-promoting least-squares migration using on-the-fly Fourier transforms. We formulate the least-squares migration objective function in the frequency domain and compute gradients for randomized subsets of shot records and frequencies, thus significantly reducing data movement and the number of overall wave equations solves. By using on-the-fly Fourier transforms, we can compute an arbitrary number of monochromatic frequency-domain wavefields with a time-domain modeling code, instead of having to solve individual Helmholtz equations for each frequency, which quickly becomes computationally infeasible when moving to high frequencies. Our numerical examples demonstrate that compressive imaging with on-the-fly Fourier transforms provides a fast and memory-efficient alternative to time-domain imaging with optimal checkpointing, whose memory requirements for a fixed background model and source wavelet is independent of the number of time steps. Instead, memory and additional computational cost grow with the number of frequencies and determine the amount of subsampling artifacts and crosstalk. In contrast to optimal checkpointing, this offers the possibility to trade both memory and computational cost for image quality or a larger number of iterations and is advantageous in new computing environments such as the cloud, where compute is often cheaper than memory and data movement.},
	keywords = {Fourier, least squares migration, sparsity-promotion},
	doi = {10.1190/geo2018-0490.1},
	url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2019/witte2018cls/witte2018cls.pdf},
	author = {Philipp A. Witte and Mathias Louboutin and Fabio Luporini and Gerard J. Gorman and Felix J. Herrmann}
}

@article{schmidt09a,
  title = 	 {Optimizing Costly Functions with Simple Constraints: A Limited-Memory Projected Quasi-Newton Algorithm},
  author = 	 {Mark Schmidt and Ewout Berg and Michael Friedlander and Kevin Murphy},
  booktitle = 	 {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {456--463},
  year = 	 {2009},
  editor = 	 {David van Dyk and Max Welling},
  volume = 	 {5},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v5/schmidt09a/schmidt09a.pdf},
  url = 	 {http://proceedings.mlr.press/v5/schmidt09a.html},
  abstract = 	 {An optimization algorithm for minimizing a smooth function over a    convex set is described.  Each iteration of the method computes a    descent direction by minimizing, over the original constraints, a    diagonal plus low-rank quadratic approximation to the function.  The    quadratic approximation is constructed using a limited-memory    quasi-Newton update.  The method is suitable for large-scale    problems where evaluation of the function is substantially more    expensive than projection onto the constraint set.  Numerical    experiments on one-norm regularized test problems indicate that the    proposed method is competitive with state-of-the-art methods such as    bound-constrained L-BFGS and orthant-wise descent. We further show    that the method generalizes to a wide class of problems, and     substantially improves on state-of-the-art methods for problems such    as learning the structure of Gaussian graphical models and Markov random    fields.}
}


@article {vanleeuwen2015GPWEMVA,
	title = {Enabling affordable omnidirectional subsurface extended image volumes via probing},
	journal = {Geophysical Prospecting},
	volume = {65},
	number = {2},
	year = {2017},
	note = {(Geophysical Prospecting)},
	month = {03},
	pages = {385-406},
	abstract = {Image gathers as a function of subsurface offset are an important tool for the inference of rock properties and velocity analysis in areas of complex geology. Traditionally, these gathers are thought of as multidimensional correlations of the source and receiver wavefields. The bottleneck in computing these gathers lies in the fact that one needs to store, compute, and correlate these wavefields for all shots in order to obtain the desired image gathers. Therefore, the image gathers are typically only computed for a limited number of subsurface points and for a limited range of subsurface offsets, which may cause problems in complex geological areas with large geologic dips. We overcome increasing computational and storage costs of extended image volumes by introducing a formulation that avoids explicit storage and removes the customary and expensive loop over shots found in conventional extended imaging. As a result, we end up with a matrix{\textendash}vector formulation from which different image gathers can be formed and with which amplitude-versus-angle and wave-equation migration velocity analysis can be performed without requiring prior information on the geologic dips. Aside from demonstrating the formation of two-way extended image gathers for different purposes and at greatly reduced costs, we also present a new approach to conduct automatic wave-equation-based migration-velocity analysis. Instead of focusing in particular offset directions and preselected subsets of subsurface points, our method focuses every subsurface point for all subsurface offset directions using a randomized probing technique. As a consequence, we obtain good velocity models at low cost for complex models without the need to provide information on the geologic dips.},
	keywords = {AVA, migration velocity analysis, Stochastic optimization},
	doi = {10.1111/1365-2478.12418},
	url = {https://slim.gatech.edu/Publications/Public/Journals/GeophysicalProspecting/2016/vanleeuwen2015GPWEMVA/vanleeuwen2015GPWEMVA.pdf},
	author = {Tristan van Leeuwen and Rajiv Kumar and Felix J. Herrmann}
}

@article {Aravkin11TRridr,
	title = {Robust inversion, dimensionality reduction, and randomized sampling},
	journal = {Mathematical Programming},
	volume = {134},
	number = {1},
	year = {2012},
	month = {08},
	pages = {101-125},
	abstract = {We consider a class of inverse problems in which the forward model is the solution operator to linear ODEs or PDEs. This class admits several dimensionality-reduction techniques based on data averaging or sampling, which are especially useful for large-scale problems. We survey these approaches and their connection to stochastic optimization. The data-averaging approach is only viable, however, for a least-squares misfit, which is sensitive to outliers in the data and artifacts unexplained by the forward model. This motivates us to propose a robust formulation based on the Student{\textquoteright}s t-distribution of the error. We demonstrate how the corresponding penalty function, together with the sampling approach, can obtain good results for a large-scale seismic inverse problem with 50 \% corrupted data.},
	keywords = {FWI, Inverse problems, Optimization, Robust estimation, Seismic inversion, Stochastic optimization},
	doi = {10.1007/s10107-012-0571-6},
	url = {http://www.springerlink.com/content/35rwr101h5736340/},
	url2 = {https://slim.gatech.edu/Publications/Public/Journals/MathematicalProgramming/aravkin2012MPrid/aravkin2012MPrid.pdf},
	author = {Aleksandr Y. Aravkin and Michael P. Friedlander and Felix J. Herrmann and Tristan van Leeuwen}
}

@article {yang2020lrpo,
	title = {Low-rank representation of omnidirectional subsurface extended image volumes},
	journal = {Geophysics},
	volume = {86},
	number = {3},
	year = {2021},
	note = {(Geophysics)},
	month = {1},
	pages = {1-41},
	abstract = {Subsurface-offset gathers play an increasingly important role in seismic imaging. These gathers are used during velocity model building and inversion of rock properties from amplitude variations. While powerful, these gathers come with high computational and storage demands to form and manipulate these high dimensional objects. This explains why only limited numbers of image gathers are computed over a limited offset range. We avoid these high costs by working with highly compressed low-rank factorizations. We arrive at these factorizations via a combination of probings with the double two-way wave equation and randomized singular value decompositions. In turn, the resulting factorizations give us access to all subsurface offsets without having to form the full extended image volumes. The latter is computationally prohibitive because extended image volumes are quadratic in image size. As a result, we can easily handle situations where conventional horizontal offset gathers are no longer focused. More importantly, the factorization also provides a mechanism to use the invariance relation of extended image volumes for velocity continuation. With this technique, extended image volumes for one background velocity model can directly be mapped to those of another background velocity model. Our low-rank factorization inherits this invariance property so we incur factorization costs only once when examining different imaging scenarios. Because all imaging experiments only involve the factors, they are computationally cheap with costs that scale with the rank of the factorization. We validate our methodology on 2D synthetics including a challenging imaging example with salt. Our experiments show that our low-rank factorization parameterizes extended image volumes naturally. Instead of brute force explicit cross-correlations between shifted source and receiver wavefields, our approach relies on the underlying linear-algebra structure that enables us to work with these objects without incurring unfeasible demands on computation and storage.},
	keywords = {extended image volumes, invariance relationship, low rank, power schemes, randomized linear algebra},
	doi = {10.1190/geo2020-0152.1},
	url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2021/yang2020lrpo/Paper_final.html},
	author = {Mengmeng Yang and Marie Graff and Rajiv Kumar and Felix J. Herrmann}
}

@article{Griewank,
 author = {Griewank, Andreas and Walther, Andrea},
 title = {Algorithm 799: Revolve: An Implementation of Checkpointing for the Reverse or Adjoint Mode of Computational Differentiation},
 journal = {ACM Trans. Math. Softw.},
 issue_date = {March 2000},
 volume = {26},
 number = {1},
 month = mar,
 year = {2000},
 issn = {0098-3500},
 pages = {19--45},
 numpages = {27},
 url = {http://doi.acm.org/10.1145/347837.347846},
 doi = {10.1145/347837.347846},
 acmid = {347846},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adjoint mode, checkpointing, computational differentiation, reverse mode},
}

@article{Symes2007,
	Author = {Symes},
	Doi = {10.1190/1.2742686},
	Eprint = {http://library.seg.org/doi/pdf/10.1190/1.2742686},
	Journal = {GEOPHYSICS},
	Number = {5},
	Pages = {SM213-SM221},
	Title = {Reverse Time Migration with Optimal Checkpointing},
	Url = {http://library.seg.org/doi/abs/10.1190/1.2742686},
	Volume = {72},
	Year = {2007},
	Bdsk-Url-1 = {http://library.seg.org/doi/abs/10.1190/1.2742686},
	Bdsk-Url-2 = {http://dx.doi.org/10.1190/1.2742686}}

@Article{kukrejacomp,
AUTHOR = {Kukreja, N. and H\"uckelheim, J. and Louboutin, M. and Washbourne, J. and Kelly, P. H. J. and Gorman, G. J.},
TITLE = {Lossy Checkpoint Compression in Full Waveform Inversion},
JOURNAL = {Geoscientific Model Development Discussions},
VOLUME = {2020},
YEAR = {2020},
PAGES = {1--26},
URL = {https://gmd.copernicus.org/preprints/gmd-2020-325/},
DOI = {10.5194/gmd-2020-325}
}

@article {McMechan,
author = {McMechan, G. A.},
title = {MIGRATION BY EXTRAPOLATION OF TIME-DEPENDENT BOUNDARY VALUES},
journal = {Geophysical Prospecting},
volume = {31},
number = {3},
publisher = {Blackwell Publishing Ltd},
issn = {1365-2478},
url = {http://dx.doi.org/10.1111/j.1365-2478.1983.tb01060.x},
doi = {10.1111/j.1365-2478.1983.tb01060.x},
pages = {413--420},
year = {1983},
}

@article{Mittet,
author = {Rune Mittet},
title = {Implementation of the Kirchhoff integral for elastic waves in staggered-grid modeling schemes},
journal = {GEOPHYSICS},
volume = {59},
number = {12},
pages = {1894-1901},
year = {1994},
doi = {10.1190/1.1443576},

URL = {
        http://dx.doi.org/10.1190/1.1443576
},
eprint = {
        http://dx.doi.org/10.1190/1.1443576
}

}

@article{RaknesR45,
	author = {Raknes, Espen Birger and Weibull, Wiktor},
	title = {Efficient 3D elastic full-waveform inversion using wavefield reconstruction methods},
	volume = {81},
	number = {2},
	pages = {R45--R55},
	year = {2016},
	doi = {10.1190/geo2015-0185.1},
	publisher = {Society of Exploration Geophysicists},
	abstract = {In reverse time migration (RTM) or full-waveform inversion (FWI), forward and reverse time propagating wavefields are crosscorrelated in time to form either the image condition in RTM or the misfit gradient in FWI. The crosscorrelation condition requires both fields to be available at the same time instants. For large-scale 3D problems, it is not possible, in practice, to store snapshots of the wavefields during forward modeling due to extreme storage requirements. We have developed an approximate wavefield reconstruction method that uses particle velocity field recordings on the boundaries to reconstruct the forward wavefields during the computation of the reverse time wavefields. The method is computationally effective and requires less storage than similar methods. We have compared the reconstruction method to a boundary reconstruction method that uses particle velocity and stress fields at the boundaries and to the optimal checkpointing method. We have tested the methods on a 2D vertical transversely isotropic model and a large-scale 3D elastic FWI problem. Our results revealed that there are small differences in the results for the three methods.},
	issn = {0016-8033},
	URL = {http://geophysics.geoscienceworld.org/content/81/2/R45},
	eprint = {http://geophysics.geoscienceworld.org/content/81/2/R45.full.pdf},
	journal = {Geophysics}
}

@misc{Sirgue2010,
	title={System and method for 3D frequency domain waveform inversion based on 3D time-domain forward modeling},
	author={Sirgue, L. and Etgen, J.T. and Albertin, U. and Brandsberg-Dahl, S.},
	url={http://www.google.ca/patents/US7725266},
	year={2010},
	month=may,
	publisher={Google Patents},
	note={US Patent 7,725,266}
}

@article{Nihei2007,
	author = {Kurt T. Nihei and Xiaoye Li},
	title = {Frequency response modelling of seismic waves using finite difference time domain with phase sensitive detection (TD–PSD)},
	journal = {Geophysical Journal International},
	year = {2007},
	volume = {169},
	number = {3},
	pages = {1069-1078},
	keywords = {finite‐difference methods, numerical techniques, seismic modelling},
	doi = {10.1111/j.1365-246X.2006.03262.x},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-246X.2006.03262.x},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-246X.2006.03262.x}
}

@article {li2015GEOPmgn,
	title = {Modified Gauss-Newton full-waveform inversion explained{\textendash}-why sparsity-promoting updates do matter},
	journal = {Geophysics},
	volume = {81},
	number = {3},
	year = {2016},
	note = {(Geophysics)},
	month = {05},
	pages = {R125-R138},
	abstract = {Full-waveform inversion can be formulated as a nonlinear least-squares optimization problem. This non-convex problem can be extremely computationally expensive because it requires repeatedly solving large linear systems that correspond to discretized partial differential equations. Randomized subsampling techniques allow us to work with small subsets of (monochromatic) source experiments, reducing the computational cost. However, this subsampling weakens subsurface illumination and introduces subsampling related incoherent artifacts. These subsampling-related artifacts{\textendash}-in conjunction with local minima that are known to plague full-waveform inversion{\textendash}-motivate us to come up with a technique to "regularize" this problem. Following earlier work, we take advantage of the fact that curvelets represent subsurface models and perturbations parsimoniously. At first impulse promoting sparsity on the model directly seems the most natural way to proceed, but we will demonstrate that in certain cases it can be advantageous to promote sparsity on the Gauss-Newton updates instead. While constraining the one-norm of the descent directions does not change not change the underlying full-waveform inversion objective, the constrained model updates remain descent directions, remove subsampling-related artifacts and improve the overall inversion result. We empirically observe this phenomenon in situations where the different model updates occur at roughly the same locations in the curvelet domain. We further investigate and analyze this phenomenon, where nonlinear inversions benefit from sparsity-promoting constraints on the updates, by means of a set of carefully selected examples including phase retrieval and full-waveform inversion. In all cases, we observe a faster decay of the residual and model error as a function of the number of iterations.},
	keywords = {Full-waveform inversion, Gauss-Newton method, sparsity promotion},
	doi = {10.1190/geo2015-0266.1},
	url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2016/li2015GEOPmgn/li2015GEOPmgn.pdf},
	author = {Xiang Li and Ernie Esser and Felix J. Herrmann}
}


@article{Faqi,
author = {Faqi Liu and Guanquan Zhang and Scott A. Morton and Jacques P. Leveille},
title = {An effective imaging condition for reverse-time migration using wavefield decomposition},
journal = {GEOPHYSICS},
volume = {76},
number = {1},
pages = {S29-S39},
year = {2011},
doi = {10.1190/1.3533914},

URL = { 
        https://doi.org/10.1190/1.3533914
    
},
eprint = { 
        https://doi.org/10.1190/1.3533914
    
}
,
    abstract = { Reverse-time migration (RTM) exhibits great superiority over other imaging algorithms in handling steeply dipping structures and complicated velocity models. However, low-frequency, high-amplitude noises commonly seen in a typical RTM image have been one of the major concerns because they can seriously contaminate the signals in the image if they are not handled properly. We propose a new imaging condition to effectively and efficiently eliminate these specific noises from the image. The method works by first decomposing the source and receiver wavefields to their one-way propagation components, followed by applying a correlation-based imaging condition to the appropriate combinations of the decomposed wavefields. We first give the physical explanation of the principle of such noises in the conventional RTM image. Then we provide the detailed mathematical theory for the new imaging condition. Finally, we propose an efficient scheme for its numerical implementation. It replaces the computationally intensive decomposition with the cost-effective Hilbert transform, which significantly improves the efficiency of the imaging condition. Applications to various synthetic and real data sets demonstrate that this new imaging condition can effectively remove the undesired low-frequency noises in the image. }
}



@article{witteisic,
   author = "Witte, P.A. and Yang, M. and Herrmann, F.J.",
   title = "Sparsity-Promoting Least-Squares Migration with the Linearized Inverse Scattering Imaging Condition", 
   journal= "",
   year = "2017",
   volume = "2017",
   number = "1",
   pages = "1-5",
   doi = "https://doi.org/10.3997/2214-4609.201701125",
   url = "https://www.earthdoc.org/content/papers/10.3997/2214-4609.201701125",
   publisher = "European Association of Geoscientists &amp; Engineers",
   issn = "2214-4609",
   type = "",
  }

@inbook{Whitmore,
author = {N. D. Whitmore and Sean Crawley},
title = {Applications of RTM inverse scattering imaging conditions},
booktitle = {SEG Technical Program Expanded Abstracts 2012},
chapter = {},
pages = {1-6},
year = {2012},
doi = {10.1190/segam2012-0779.1},
URL = {https://library.seg.org/doi/abs/10.1190/segam2012-0779.1},
eprint = {https://library.seg.org/doi/pdf/10.1190/segam2012-0779.1},
    abstract = { Reverse time migration (RTM) is achieved by a forward and reverse time propagation of source and receiver wavefields respectively, followed by an imaging condition. The quality of the image from any RTM implementation depends directly on the method's ability to separate the true reflection data from the backscattered correlation noise between the source and receiver wavefields. In this paper we demonstrate the application of an inverse scattering imaging condition that significantly reduces this backscattered correlation noise and results in a much higher quality subsurface image than is achieved by typical RTM imaging conditions. }
}

@ARTICLE{hutchpp,
       author = {{Meyer}, Raphael A. and {Musco}, Cameron and {Musco}, Christopher and {Woodruff}, David P.},
        title = "{Hutch++: Optimal Stochastic Trace Estimation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Mathematics - Numerical Analysis},
         year = 2020,
        month = oct,
          eid = {arXiv:2010.09649},
        pages = {arXiv:2010.09649},
archivePrefix = {arXiv},
       eprint = {2010.09649},
 primaryClass = {cs.DS},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv201009649M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Avron,
author = {Avron, Haim and Toledo, Sivan},
title = {Randomized Algorithms for Estimating the Trace of an Implicit Symmetric Positive Semi-Definite Matrix},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/1944345.1944349},
doi = {10.1145/1944345.1944349},
abstract = {We analyze the convergence of randomized trace estimators. Starting at 1989, several algorithms have been proposed for estimating the trace of a matrix by 1/MΣi=1M ziT Azi, where the zi are random vectors; different estimators use different distributions for the zis, all of which lead to E(1/MΣi=1M ziT Azi) = trace(A). These algorithms are useful in applications in which there is no explicit representation of A but rather an efficient method compute zTAz given z. Existing results only analyze the variance of the different estimators. In contrast, we analyze the number of samples M required to guarantee that with probability at least 1-δ, the relative error in the estimate is at most ϵ. We argue that such bounds are much more useful in applications than the variance. We found that these bounds rank the estimators differently than the variance; this suggests that minimum-variance estimators may not be the best.We also make two additional contributions to this area. The first is a specialized bound for projection matrices, whose trace (rank) needs to be computed in electronic structure calculations. The second is a new estimator that uses less randomness than all the existing estimators.},
journal = {J. ACM},
month = apr,
articleno = {8},
numpages = {34},
keywords = {Trace estimation, implicit linear operators}
}

@article {witte2019TPDedas,
	title = {An Event-Driven Approach to Serverless Seismic Imaging in the Cloud},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	volume = {31},
	number = {9},
	year = {2020},
	note = {(IEEE Transactions on Parallel and Distributed Systems)},
	month = {03},
	pages = {2032-2049},
	abstract = {Adapting the cloud for high-performance computing (HPC) is a challenging task, as software for HPC applications hinges on fast network connections and is sensitive to hardware failures. Using cloud infrastructure to recreate conventional HPC clusters is therefore in many cases an infeasible solution for migrating HPC applications to the cloud. As an alternative to the generic lift and shift approach, we consider the specific application of seismic imaging and demonstrate a serverless and event-driven pproach for running large-scale instances of this problem in the cloud. Instead of permanently running compute instances, our workflow is based on a serverless architecture with high throughput batch computing and event-driven computations, in which computational resources are only running as long as they are utilized. We demonstrate that this approach is very flexible and allows for resilient and nested levels of parallelization, including domain decomposition for solving the underlying partial differential equations. While the event-driven approach introduces some overhead as computational resources are repeatedly restarted, it inherently provides resilience to instance shut-downs and allows a significant reduction of cost by avoiding idle instances, thus making the cloud a viable alternative to on-premise clusters for large-scale seismic imaging.},
	keywords = {cloud, event-driven, Imaging, LSRTM, serverless},
	doi = {10.1109/TPDS.2020.2982626},
	url = {https://slim.gatech.edu/Publications/Public/Journals/IEEETPDS/2020/witte2019TPDedas/witte2019TPDedas.html},
	author = {Philipp A. Witte and Mathias Louboutin and Henryk Modzelewski and Charles Jones and James Selvage and Felix J. Herrmann}
}

@article{romero,
author = {Louis A. Romero and Dennis C. Ghiglia and Curtis C. Ober and Scott A. Morton},
title = {Phase encoding of shot records in prestack migration},
journal = {GEOPHYSICS},
volume = {65},
number = {2},
pages = {426-436},
year = {2000},
doi = {10.1190/1.1444737},
URL = {https://doi.org/10.1190/1.1444737},
eprint = {https://doi.org/10.1190/1.1444737},
    abstract = { Frequency‐domain shot‐record migration can produce higher quality images than Kirchhoff migration but typically at a greater cost. The computing cost of shot‐record migration is the product of the number of shots in the survey and the expense of each individual migration. Many attempts to reduce this cost have focused on the speed of the individual migrations, trying to achieve a better trade‐off between accuracy and speed. Another approach is to reduce the number of migrations. We investigate the simultaneous migration of shot records using frequency‐domain shot‐record migration algorithms. The difficulty with this approach is the production of so‐called crossterms between unrelated shot and receiver wavefields, which generate unwanted artifacts or noise in the final image. To reduce these artifacts and obtain an image comparable in quality to the single‐shot‐per‐migration result, we have introduced a process called phase encoding, which shifts or disperses these crossterms. The process of phase encoding thus allows one to trade S/N ratio for the speed of migrating the entire survey. Several encoding functions and two application strategies have been tested. The first strategy, combining multiple shots per migration and using each shot only once, reduces computation in direct relation to the number of shots combined. The second strategy, performing multiple migrations of all the shots in the survey, provides a means to reduce the crossterm noise by stacking the resulting images. The additional noise in both strategies may be tolerated if it is no stronger than the inherent seismic noise in the migrated image and if the final image is achieved with less cost. }
}

@article{krebs,
author = {Jerome R. Krebs and John E. Anderson and David Hinkley and Ramesh Neelamani and Sunwoong Lee and Anatoly Baumstein and Martin-Daniel Lacasse},
title = {Fast full-wavefield seismic inversion using encoded sources},
journal = {GEOPHYSICS},
volume = {74},
number = {6},
pages = {WCC177-WCC188},
year = {2009},
doi = {10.1190/1.3230502},
URL = {https://doi.org/10.1190/1.3230502},
eprint = {https://doi.org/10.1190/1.3230502 },
    abstract = { Full-wavefield seismic inversion (FWI) estimates a subsurface elastic model by iteratively minimizing the difference between observed and simulated data. This process is extremely computationally intensive, with a cost comparable to at least hundreds of prestack reverse-time depth migrations. When FWI is applied using explicit time-domain or frequency-domain iterative-solver-based methods, the seismic simulations are performed for each seismic-source configuration individually. Therefore, the cost of FWI is proportional to the number of sources. We have found that the cost of FWI for fixed-spread data can be significantly reduced by applying it to data formed by encoding and summing data from individual sources. The encoding step forms a single gather from many input source gathers. This gather represents data that would have been acquired from a spatially distributed set of sources operating simultaneously with different source signatures. The computational cost of FWI using encoded simultaneous-source gathers is reduced by a factor roughly equal to the number of sources. Further, this efficiency is gained without significantly reducing the accuracy of the final inverted model. The efficiency gain depends on subsurface complexity and seismic-acquisition parameters. There is potential for even larger improvements of processing speed. }
}

@article{haberse,
author = {Eldad Haber and Kees van den Doel and Lior Horesh},
title = {Optimal design of simultaneous source encoding},
journal = {Inverse Problems in Science and Engineering},
volume = {23},
number = {5},
pages = {780-797},
year  = {2015},
publisher = {Taylor & Francis},
doi = {10.1080/17415977.2014.934821},
URL = {https://doi.org/10.1080/17415977.2014.934821},
eprint = {https://doi.org/10.1080/17415977.2014.934821}
}

@article{Moghaddamsefwi,
author = {Peyman P. Moghaddam and Henk Keers and Felix J. Herrmann and Wim A. Mulder},
title = {A new optimization approach for source-encoding full-waveform inversion},
journal = {GEOPHYSICS},
volume = {78},
number = {3},
pages = {R125-R132},
year = {2013},
doi = {10.1190/geo2012-0090.1},
URL = {https://doi.org/10.1190/geo2012-0090.1},
eprint = {https://doi.org/10.1190/geo2012-0090.1},
    abstract = { ABSTRACTWaveform inversion is the method of choice for determining a highly heterogeneous subsurface structure. However, conventional waveform inversion requires that the wavefield for each source is computed separately. This makes it very expensive for realistic 3D seismic surveys. Source-encoding waveform inversion, in which the sources are modeled simultaneously, is considerably faster than conventional waveform inversion but suffers from artifacts. These artifacts can partly be removed by assigning random weights to the source wavefields. We found that the misfit function, and therefore also its gradient, for source-encoding waveform inversion is an unbiased random estimation of the misfit function used in conventional waveform inversion. We found a new method of source-encoding waveform inversion that takes into account the random nature of the gradients used in the optimization. In this new method, the gradient at each iteration is a weighted average of past gradients such that the most recent gradients have the largest weights with exponential decay. This way we damped the random fluctuations of the gradient by incorporating information from the previous iterations. We compared this new method with existing source-encoding waveform inversion methods as well as conventional waveform inversion and found that the model misfit reduction is faster and smoother than those of existing source-encoding waveform inversion methods, and it approaches the model misfit reduction obtained in conventional waveform inversion. }
}

@article{leeuwenfwi,
author = {van Leeuwen, Tristan and Herrmann, Felix J.},
title = {Fast waveform inversion without source-encoding},
journal = {Geophysical Prospecting},
volume = {61},
number = {s1},
pages = {10-19},
keywords = {Full-waveform inversion, Source-encoding, Stochastic optimization},
doi = {https://doi.org/10.1111/j.1365-2478.2012.01096.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2478.2012.01096.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2478.2012.01096.x},
abstract = {ABSTRACT Randomized source-encoding has recently been proposed as a way to dramatically reduce the costs of full waveform inversion. The main idea is to replace all sequential sources by a small number of simultaneous sources. This introduces random cross-talk in model updates and special stochastic optimization strategies are required to deal with this. Two problems arise with this approach: i) source-encoding can only be applied to fixed-spread acquisition setups and ii) stochastic optimization methods tend to converge very slowly, relying on averaging to suppress the cross-talk. Although the slow convergence is partly off-set by a low iteration cost, we show that conventional optimization strategies are bound to outperform stochastic methods in the long run. In this paper we argue that we do not need randomized source-encoding to reap the benefits of stochastic optimization and we review an optimization strategy that combines the benefits of both conventional and stochastic optimization. The method uses a gradually increasing batch of sources. Thus, iterations are initially very cheap and this allows the method to make fast progress in the beginning. As the batch-size grows, the method behaves like conventional optimization, allowing for fast convergence. Stylized numerical examples suggest that the stochastic and hybrid methods perform equally well with and without source-encoding and that the hybrid method outperforms both conventional and stochastic optimization. The method does not rely on source-encoding techniques and can thus be applied to marine data. We illustrate this on a realistic synthetic model.},
year = {2013}
}

@article {peters2018pmf,
	title = {Projection methods and applications for seismic nonlinear inverse problems with multiple constraints},
	journal = {Geophysics},
	volume = {84},
	number = {2},
	year = {2018},
	note = {(Geophysics)},
	month = {02},
	pages = {R251-R269},
	abstract = {Nonlinear inverse problems are often hampered by non-uniqueness and local minima because of missing low frequencies and far offsets in the data, lack of access to good starting models, noise, and modeling errors. A well-known approach to counter these deficiencies is to include prior information on the unknown model, which regularizes the inverse problem. While conventional regularization methods have resulted in enormous progress in ill-posed (geophysical) inverse problems, challenges remain when the prior information consists of multiple pieces. To handle this situation, we propose an optimization framework that allows us to add multiple pieces of prior information in the form of constraints. Compared to additive regularization penalties, constraints have a number of advantages making them more suitable for inverse problems such as full-waveform inversion. The proposed framework is rigorous because it offers assurances that multiple constraints are imposed uniquely at each iteration, irrespective of the order in which they are invoked. To project onto the intersection of multiple sets uniquely, we employ Dykstra{\textquoteright}s algorithm that scales to large problems and does not rely on trade-off parameters. In that sense, our approach differs substantially from approaches such as Tikhonov regularization, penalty methods, and gradient filtering. None of these offer assurances, which makes them less suitable to full-waveform inversion where unrealistic intermediate results effectively derail the iterative inversion process. By working with intersections of sets, we keep expensive objective and gradient calculations unaltered, separate from projections, and we also avoid trade-off parameters. These features allow for easy integration into existing code bases. In addition to more predictable behavior, working with constraints also allows for heuristics where we built up the complexity of the model gradually by relaxing the constraints. This strategy helps to avoid convergence to local minima that represent unrealistic models. We illustrate this unique feature with examples of varying complexity.},
	keywords = {constraints, Full-waveform inversion, intersection, Optimization, projection, regularization},
	doi = {10.1190/geo2018-0192.1},
	url = {https://slim.gatech.edu/Publications/Public/Journals/Geophysics/2018/peters2018pmf/peters2018pmf.html},
	author = {Bas Peters and Brendan R. Smithyman and Felix J. Herrmann}
}

@article {esser2016tvr,
	title = {Total-variation regularization strategies in full-waveform inversion},
	journal = {SIAM Journal on Imaging Sciences},
	volume = {11},
	number = {1},
	year = {2018},
	note = {(SIAM Journal on Imaging Sciences)},
	pages = {376-406},
	abstract = {We propose an extended full-waveform inversion formulation that includes general convex constraints on the model. Though the full problem is highly nonconvex, the overarching optimization scheme arrives at geologically plausible results by solving a sequence of relaxed and warm-started constrained convex subproblems. The combination of box, total-variation, and successively relaxed asymmetric total-variation constraints allows us to steer free from parasitic local minima while keeping the estimated physical parameters laterally continuous and in a physically realistic range. For accurate starting models, numerical experiments carried out on the challenging 2004 BP velocity benchmark demonstrate that bound and total-variation constraints improve the inversion result significantly by removing inversion artifacts, related to source encoding, and by clearly improved delineation of top, bottom, and flanks of a high-velocity high-contrast salt inclusion. The experiments also show that for poor starting models these two constraints by themselves are insufficient to detect the bottom of high-velocity inclusions such as salt. Inclusion of the one-sided asymmetric total-variation constraint overcomes this issue by discouraging velocity lows to buildup during the early stages of the inversion. To the author{\textquoteright}s knowledge the presented algorithm is the first to successfully remove the imprint of local minima caused by poor starting models and band-width limited finite aperture data.},
	keywords = {constrained optimization, Full-waveform inversion, hinge loss, salt, total variation},
	doi = {10.1137/17M111328X},
	url = {https://slim.gatech.edu/Publications/Public/Journals/SIAMJournalOnImagingSciences/2018/esser2016tvr/esser2016tvr.pdf},
	url2 = {https://doi.org/10.1137/17M111328X},
	author = {Ernie Esser and Llu{\'\i}s Guasch and Tristan van Leeuwen and Aleksandr Y. Aravkin and Felix J. Herrmann}
}

@article{halkorand,
author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
title = {Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions},
journal = {SIAM Review},
volume = {53},
number = {2},
pages = {217-288},
year = {2011},
doi = {10.1137/090771806},
URL = {https://doi.org/10.1137/090771806},
eprint = {https://doi.org/10.1137/090771806}
}

@misc{graff2017SINBADFlrp,
	title = {Low-rank representation of omnidirectional subsurface extended image volumes},
	year = {2017},
	publisher = {SINBAD},
	abstract = {Extended image volumes are an important migration tool in seismic exploration. However the computation and the storage of omnidirectional subsurface extended image volumes are usually prohibitive. That is why some solutions have been already proposed for instance by focusing on horizontal offsets only. In our work, we will consider a linear algebra approach to deal with the low-rank representation of extended image volumes with full offsets. We will never build entirely the resulting matrix but get only actions of it on well-chosen probing vectors, based on Low-Rank decomposition or randomized SVD. This representation allows us to have access to all the energy of the extended image volume matrix and still limits the storage of the information and the computational cost.},
	keywords = {Presentation, SINBAD, SINBADFALL2017, SLIM},
	url = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/graff2017SINBADFlrp/graff2017SINBADFlrp.pdf},
	url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SINBAD/2017/Fall/graff2017SINBADFlrp/graff2017SINBADFlrp.mov},
	author = {Marie Graff-Kray and Rajiv Kumar and Felix J. Herrmann}
}

@article{friedlander,
author = {Friedlander, Michael P. and Schmidt, Mark},
title = {Hybrid Deterministic-Stochastic Methods for Data Fitting},
journal = {SIAM Journal on Scientific Computing},
volume = {34},
number = {3},
pages = {A1380-A1405},
year = {2012},
doi = {10.1137/110830629},
URL = { https://doi.org/10.1137/110830629},
eprint = {https://doi.org/10.1137/110830629}
}

@article{witteJUDI2019,
author = {Philipp A. Witte and Mathias Louboutin and Navjot Kukreja and Fabio Luporini and Michael Lange and Gerard J. Gorman and Felix J. Herrmann},
title = {A large-scale framework for symbolic implementations of seismic inversion algorithms in Julia},
journal = {GEOPHYSICS},
volume = {84},
number = {3},
pages = {F57-F71},
year = {2019},
doi = {10.1190/geo2018-0174.1},
URL = {https://doi.org/10.1190/geo2018-0174.1},
eprint = {https://doi.org/10.1190/geo2018-0174.1}
}

@conference{louboutin2021SEGulm,
	title = {Ultra-low memory seismic inversion with randomized trace estimation},
	booktitle = {SEG Technical Program Expanded Abstracts},
	year = {2021},
	note = {(IMAGE, Denver)},
	month = {09},
	pages = {787-791},
	abstract = {Inspired by recent work on extended image volumes that lays the ground for randomized probing of extremely large seismic wavefield matrices, we present a memory frugal and computationally efficient inversion methodology that uses techniques from randomized linear algebra. By means of a carefully selected realistic synthetic example, we demonstrate that we are capable of achieving competitive inversion results at a fraction of the memory cost of conventional full-waveform inversion with limited computational overhead. By exchanging memory for negligible computational overhead, we open with the presented technology the door towards the use of low-memory accelerators such as GPUs.},
	keywords = {FWI, HPC, inversion, randomized linear algebra, SEG},
	doi = {10.1190/segam2021-3584072.1},
	url = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2021/louboutin2021SEGulm/louboutinp.html},
	url2 = {https://slim.gatech.edu/Publications/Public/Conferences/SEG/2021/louboutin2021SEGulm/louboutin2021SEGulm.mp4},
	software = {https://github.com/slimgroup/TimeProbeSeismic.jl},
	author = {Mathias Louboutin and Felix J. Herrmann}
}

@ARTICLE{Thomsen1986,
   author = {{Thomsen}, Leon},
    title = "{Weak elastic anisotropy}",
  journal = {Geophysics},
     year = 1986,
    month = october,
   volume = 51, 
   number = 10, 
    pages = {1964-1966}
}

@Book{trefethen1997numerical,
  title={Numerical linear algebra},
  author={Trefethen, Lloyd N and Bau III, David},
  year={1997},
  publisher={Siam},
  edition={1st}
}

@article{symesiv,
	author = {Symes, William W.},
	title = {Migration velocity analysis and waveform inversion},
	journal = {Geophysical Prospecting},
	volume = {56},
	number = {6},
	pages = {765-790},
	doi = {https://doi.org/10.1111/j.1365-2478.2008.00698.x},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2478.2008.00698.x},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2478.2008.00698.x},
	abstract = {ABSTRACT Least-squares inversion of seismic reflection waveform data can reconstruct remarkably detailed models of subsurface structure and take into account essentially any physics of seismic wave propagation that can be modelled. However, the waveform inversion objective has many spurious local minima, hence convergence of descent methods (mandatory because of problem size) to useful Earth models requires accurate initial estimates of long-scale velocity structure. Migration velocity analysis, on the other hand, is capable of correcting substantially erroneous initial estimates of velocity at long scales. Migration velocity analysis is based on prestack depth migration, which is in turn based on linearized acoustic modelling (Born or single-scattering approximation). Two major variants of prestack depth migration, using binning of surface data and Claerbout's survey-sinking concept respectively, are in widespread use. Each type of prestack migration produces an image volume depending on redundant parameters and supplies a condition on the image volume, which expresses consistency between data and velocity model and is hence a basis for velocity analysis. The survey-sinking (depth-oriented) approach to prestack migration is less subject to kinematic artefacts than is the binning-based (surface-oriented) approach. Because kinematic artefacts strongly violate the consistency or semblance conditions, this observation suggests that velocity analysis based on depth-oriented prestack migration may be more appropriate in kinematically complex areas. Appropriate choice of objective (differential semblance) turns either form of migration velocity analysis into an optimization problem, for which Newton-like methods exhibit little tendency to stagnate at nonglobal minima. The extended modelling concept links migration velocity analysis to the apparently unrelated waveform inversion approach to estimation of Earth structure: from this point of view, migration velocity analysis is a solution method for the linearized waveform inversion problem. Extended modelling also provides a basis for a nonlinear generalization of migration velocity analysis. Preliminary numerical evidence suggests a new approach to nonlinear waveform inversion, which may combine the global convergence of velocity analysis with the physical fidelity of model-based data fitting.},
	year = {2008}
}

@article{Claerboututrm,
	author = {Jon F. Claerbout},
	title = {TOWARD A UNIFIED THEORY OF REFLECTOR MAPPING},
	journal = {GEOPHYSICS},
	volume = {36},
	number = {3},
	pages = {467-481},
	year = {1971},
	doi = {10.1190/1.1440185},
	URL = { https://doi.org/10.1190/1.1440185},
	eprint = {https://doi.org/10.1190/1.1440185},
	abstract = { Schemes for seismic mapping of reflectors in the presence of an arbitrary velocity model, dipping and curved reflectors, diffractions, ghosts, surface elevation variations, and multiple reflections are reviewed and reduced to a single formula involving up and downgoing waves. The mapping formula may be implemented without undue complexity by means of difference approximations to the relativistic Schroedinger equation. }
}


@article{Claerboutdcm,
    author = {Jon F. Claerbout and Stephen M. Doherty},
	title = {DOWNWARD CONTINUATION OF MOVEOUT‐CORRECTED SEISMOGRAMS},
	journal = {GEOPHYSICS},
	volume = {37},
	number = {5},
	pages = {741-768},
	year = {1972},
	doi = {10.1190/1.1440298},
	URL = {https://doi.org/10.1190/1.1440298},
	eprint = {https://doi.org/10.1190/1.1440298},
	abstract = { Earlier work developed a method of migration of seismic data based on numerical solutions of partial differential equations. The method was designed for the geometry of a single source with a line of surface receivers. Here the method is extended to the geometry of stacked sections, or what is nearly the same thing, to the geometry where a source and receiver move together along the surface as in marine profiling. The basic idea simply stated is that the best receiver line for any reflector is just at (or above) the reflector. Data received at a surface line of receivers may be extrapolated by computer to data at a hypothetical receiver line at any depth. By considering migration before stacking over offset, it is found that certain ambiguities in velocity analysis may be avoided. }
}

@book{segseamI,
    author = {Fehler, Michael and Keliher, P. Joseph},
    title = "{SEAM Phase I: Challenges of Subsalt Imaging in Tertiary Basins, with Emphasis on Deepwater Gulf of Mexico}",
    publisher = {Society of Exploration Geophysicists},
    year = {2011},
    month = {01},
    abstract = "{“SEAM is a collaborative industrial research effort dedicated to largescale, geophysical numerical simulation projects. The projects are designed to provide the geosciences exploration community with earth models and simulated data that represent significant geophysical challenges of high business value to the petroleum resource industry. The Phase I project produced a deepwater subsalt earth model designed to capture as much physics and realism as possible in a 3D model that was relevant to oil and gas exploration. The 3D model covers a 40 õ 35 õ 15 km area and includes a complex salt intrusive in a folded Tertiary basin. The primary deliverable was the seismic data set of variable density acoustic simulations consisting of 200 TB of uncompressed traces for over 60,000 shots. Also delivered to the participants were several smaller compressed subsets of these data (“classic” data sets) intended for easier handling, simpler distribution to third parties, and easier comparison of imaging tests results. This report covers how the prime objectives of Phase I were met. Details are outlined in chapters on Model Development, Numerical Design and Vendor Qualification, Acquisition Design, Production Simulations, Quality Control, and Data Storage and Distribution.”“SEAM is a collaborative industrial research effort dedicated to largescale, geophysical numerical simulation projects. The projects are designed to provide the geosciences exploration community with earth models and simulated data that represent significant geophysical challenges of high business value to the petroleum resource industry. The Phase I project produced a deepwater subsalt earth model designed to capture as much physics and realism as possible in a 3D model that was relevant to oil and gas exploration. The 3D model covers a 40 õ 35 õ 15 km area and includes a complex salt intrusive in a folded Tertiary basin. The primary deliverable was the seismic data set of variable density acoustic simulations consisting of 200 TB of uncompressed traces for over 60,000 shots. Also delivered to the participants were several smaller compressed subsets of these data (“classic” data sets) intended for easier handling, simpler distribution to third parties, and easier comparison of imaging tests results. This report covers how the prime objectives of Phase I were met. Details are outlined in chapters on Model Development, Numerical Design and Vendor Qualification, Acquisition Design, Production Simulations, Quality Control, and Data Storage and Distribution.”}",
    isbn = {9781560802877},
    doi = {10.1190/1.9781560802945},
    url = {https://doi.org/10.1190/1.9781560802945},
}

@inbook{krebsse,
author = {Jerome R. Krebs and John E. Anderson and David Hinkley and Anatoly Baumstein and Sunwoong Lee and Ramesh Neelamani and Martin‐Daniel Lacasse},
title = {Fast full wave seismic inversion using source encoding},
booktitle = {SEG Technical Program Expanded Abstracts 2009},
chapter = {},
pages = {2273-2277},
year = {2009},
doi = {10.1190/1.3255314},
URL = {https://library.seg.org/doi/abs/10.1190/1.3255314},
eprint = {https://library.seg.org/doi/pdf/10.1190/1.3255314},
    abstract = { Full Wavefield Seismic Inversion (FWI) estimates a subsurface elastic model by iteratively minimizing the difference between observed and simulated data. This process is extremely compute intensive, with a cost on the order of at least hundreds of prestack reverse time migrations. For time‐domain and Krylov‐based frequency‐domain FWI, the cost of FWI is proportional to the number of seismic sources inverted. We have found that the cost of FWI can be significantly reduced by applying it to data processed by encoding and summing individual source gathers, and by changing the encoding functions between iterations. The encoding step forms a single gather from many input source gathers. This gather represents data that would have been acquired from a spatially distributed set of sources operating simultaneously with different source signatures. We demonstrate, using synthetic data, significant cost reduction by applying FWI to encoded simultaneous‐source data. }
}