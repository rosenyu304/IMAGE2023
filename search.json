[
  {
    "objectID": "OneShot/abstract.html",
    "href": "OneShot/abstract.html",
    "title": "Learned one-shot imaging",
    "section": "",
    "text": "\\[\n    \\newcommand{\\pluseq}{\\mathrel{+}=}\n\\]"
  },
  {
    "objectID": "OneShot/abstract.html#objectives-and-scope-100-words",
    "href": "OneShot/abstract.html#objectives-and-scope-100-words",
    "title": "Learned one-shot imaging",
    "section": "OBJECTIVES AND SCOPE (100 words)",
    "text": "OBJECTIVES AND SCOPE (100 words)\nSeismic imaging’s main limiting factor is the scale of the involved dataset and the number of independent wave-equations solves required for each source. We introduce a learned framework that extends the conventional computationally reductive linear source superpositions (simultaneous sources) to a non-linear learned superposition and its corresponding super-shot. With this method, we can obtain an image of the subsurface at the cost of a single shot migration by learning the most informative superposition of shots."
  },
  {
    "objectID": "OneShot/abstract.html#methods-procedures-process-250-words",
    "href": "OneShot/abstract.html#methods-procedures-process-250-words",
    "title": "Learned one-shot imaging",
    "section": "METHODS, PROCEDURES, PROCESS (250 words)",
    "text": "METHODS, PROCEDURES, PROCESS (250 words)\nSimultenous source imaging takes advantage of the linearity of the wave equation to add different shot records together reducing the cost of imaging. Because of the linearity, the same linear transformation can be applied both to the source and data to keep the problem well posed. Here, we introduce a non-linear superposition, and to compensate for the inability to apply the corrsponding transform to the source, we simultenously learn both the non-linear source and non-linear data with a neural network. This networks takes the shot records as input and outputs the learned supershot and simultenous source. The network is trained in two ways. First we train a supervised network minimizing the difference between the migrated non-linear supershot and the true model perturbation which requires access to the true model. Since in practice the true subsurface image is unknown, we train a second unsupervised network minimizing the difference between the true data and the migrated-demingrated supershot."
  },
  {
    "objectID": "OneShot/abstract.html#results-observations-conclusions-250-words",
    "href": "OneShot/abstract.html#results-observations-conclusions-250-words",
    "title": "Learned one-shot imaging",
    "section": "RESULTS, OBSERVATIONS, CONCLUSIONS (250 words)",
    "text": "RESULTS, OBSERVATIONS, CONCLUSIONS (250 words)\nWe show on Figure 1 the results with the supervised training and on Figure 2 with unsupervised trainging. We shot the learned super-shot and simultenous source next to the true observed data and compare the obtained one-shot subsurface image with standard multi-source RTM and conventional random simultenous sources imaging. We see that the image is not only less noisy that the standard simultenous source image but also a better representation of the subsurface than the conventional RTM. This demonstrate that we can obtain a good image of the earth at the cost of a single source migration while avoiding the artifacts associated with conventional simultenous source imaging."
  },
  {
    "objectID": "OneShot/abstract.html#significancenovelty-100-words",
    "href": "OneShot/abstract.html#significancenovelty-100-words",
    "title": "Learned one-shot imaging",
    "section": "SIGNIFICANCE/NOVELTY (100 words)",
    "text": "SIGNIFICANCE/NOVELTY (100 words)\nWe introduced the first instance of non-linear simuktenous source imaging. Because only linear transforms can be applied to the source and data using the linearity of the wave-equation, we introduced a neural network capable of creating a pair of simultenous source and super-shot that is most informative for migration. This learned on-shot imaging framework allows to drastically reduce the cost of seismic imaging while requiring limited training resources since only single-shot are migrated."
  },
  {
    "objectID": "OneShot/abstract.html#introduction",
    "href": "OneShot/abstract.html#introduction",
    "title": "Learned one-shot imaging",
    "section": "Introduction",
    "text": "Introduction\nbla"
  },
  {
    "objectID": "OneShot/abstract.html#methodology",
    "href": "OneShot/abstract.html#methodology",
    "title": "Learned one-shot imaging",
    "section": "Methodology",
    "text": "Methodology\nWe now introduce the formulation of aour learned simultenous source-data pair for seismic imaging. We derive two training problem where the first onbe rely on the knowledge of the true perturbation, while the second one solely rely on the observed data. Fundamentally, we are introducing a formulation that learns the most informative single super-shot and correspoinding source given either a subsurface refelctivity model or the surface recorded data.\n\nSummary networks and probabilistic symmetry\n(Deans 2002) -Summary statistics reduce the size of incoming datasets while maintainting the same posterior distribution p(x|y) = p(x|summary) (Radev et al. 2020) -Summary networks learn to reduce the size of incoming datasets and maximize informativeness of the summarized data due to joint learning of summary network and posterior learning network. -hand waves an argument that jointly trained networks will maximize the mutual information between h(y) and x (Müller et al. 2021) -Goes in to further detail and rigoursly proves that jointly trained networks will maximize the mutual information between h(y) and x (Bloem-Reddy and Teh 2020) suggests to use learned layers that are invariant under a certain transformation. This transformation is described by the probabilistic assumption on your data.\nExample paragraph: This work takes inspiration from the concept of a summary network (Radev et al. 2020) these are networks that compress observables \\(d_{obs}\\) while maximining information useful for inference of un-observables \\(x\\). To guide the architectural design of a summary network (Bloem-Reddy and Teh 2020) suggests to use learned layers that respect the probabalistic symmetry of the data. Practically, this is accomplished by making the layers be invariant under a certain transformation. For the case active source seismic imaging, i.i.d sampling entails the assumption that the order of sources does not matter. This assumption is implicit in the sum structure of RTM/gradient calculations. Therefore it is invariant with respect to permutation transformation. Our approach is most similar to this case, since we use a UNet with many input channels that output a single channel. In our testing, having a separate network for each dataset did not perform better thatn simply inputing each dataset along a channel.\n\n\nSupervised\nTHe simplest formulation aims to learn the super-shot and simultenaous source that best inform the model perturbation, given the surfac recorded data. Mathematically, it means that we are trying to fit the true mode lperturbation with two networks that learn a single super-shot and simultenous source for the Jacobian from the indicudual field recorded shot records. Mathematically, the learning can be written as: \\[\n\\min_{\\theta, \\phi} \\ \\mathbb{E}\\left[ J(\\mathcal{H}_{\\phi}(\\mathcal{G}_{\\theta}(d_{\\text{obs}}))^\\top \\mathcal{G}_{\\theta}(d_{\\text{obs}}) - \\delta m \\right]\n\\tag{1}\\]\nwhere \\(\\mathcal{H}_{\\phi}, \\mathcal{G}_{\\theta}\\) are the two networks mapping the individual shot records into a single super-shot (the learned simultenous-source is learned at the receiver locations), \\(J\\) is the conventionnal adjoint Born imaging operator, \\(d_{\\text{obs}}\\) is the observed data and \\(\\delta m\\) is the model perturbation. We note that to compute an update on the two networks simultenaously, the gradient of the Jacobian with respect to its source is necessary. This derivative is however trivial to obtain with JUDI.jl thanks to its high-level linear algebra abstraction and integration with automatic differentiation framework in Julia.\n\n\nUnsupervised\n\\[\n\\min_{\\theta, \\phi} \\ \\mathbb{E}\\left[ \\tilde{J}_{\\text{rtm}} J(\\mathcal{H}_{\\phi}(\\mathcal{G}_{\\theta}(d_{\\text{obs}}))^\\top \\mathcal{G}_{\\theta}(d_{\\text{obs}}) - \\tilde{d}_{\\text{obs}} \\right ]\n\\tag{2}\\]\nWhere \\(\\tilde{d}_{\\text{obs}} = \\sum_{i=1}^{n_src} w_i d_{\\text{obs},i}\\) is a random super shot with \\(w_i := \\mathcal{N}(0, 1)\\) and \\(\\tilde{J}\\) is the corresponding simultenous source born modeling operator. While this formulation ivolves an additional demigration (and therfore and additional migration to compute hte gradient), we do not require any knowledge of the true model perturbation but only the data. We could therefore in theory use this formulation for a wide range of datasets at once to generalize to any survey."
  },
  {
    "objectID": "OneShot/abstract.html#synthetic-case-studies",
    "href": "OneShot/abstract.html#synthetic-case-studies",
    "title": "Learned one-shot imaging",
    "section": "Synthetic case studies",
    "text": "Synthetic case studies\nWe illustrate our method on a realstic 2D imaging problem. We created a dataset of 2000 2D slices by extracting slices out of the 3D overthrust model. We then split this dataset into 1600 slices for trainng and 400 slices for testing. For each 2D slice, we generate 21 shot records. One of the main advantage of our one-shot imaging method is that we only require a single migration-demigration per iteration. Therefore, we can perform 21 epochs before arriving to a computationnal cost equivalent to the plain standard RTM imaging of each slice. Since we only perform 15 epochs, our method is overall cheapper than computing the RTM on every single shot if we include the cost of training.\nWe trained the networks, both in the supervised and unsupervised case, for 15 epochs with a learning rate of \\(.0004\\) using the Adam optimizer.\n\n\n\n\n\n\n\nData\n\n\n\n\n\n\n\nRTMs\n\n\n\n\nFigure 1: Learned super-shot and simultenous sources on a testing slice. We show a shot record and the moirated image with a random supershot and with sequential shots for reference.\n\n\n\n\n\n\n\n\n\nData\n\n\n\n\n\n\n\nRTMs\n\n\n\n\nFigure 2: Learned super-shot and simultenous sources on a testing slice. We show a shot record and the moirated image with a random supershot and with sequential shots for reference.\n\n\n\nCode availability\nTBD"
  },
  {
    "objectID": "OneShot/abstract.html#discussion-and-conclusions",
    "href": "OneShot/abstract.html#discussion-and-conclusions",
    "title": "Learned one-shot imaging",
    "section": "Discussion and conclusions",
    "text": "Discussion and conclusions\nWe introduced data-domain learning method that provides high accuracy imagies of the subsurface through one-shot imaging. We trained a network that learns the simultenous source and super-shot that most inform the subsurface from the field recorded data. We showed that we obtain high accuracy images of the subsurface that contain broader frequency range than standard imaging and does not require prior knowledge of the source. Additionnally, the overall computationnal cost of training does not exceed the traditionnal cost of imaging."
  },
  {
    "objectID": "OneShot/abstract.html#acknowledgement",
    "href": "OneShot/abstract.html#acknowledgement",
    "title": "Learned one-shot imaging",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis research was carried out with the support of Georgia Research Alliance and partners of the ML4Seismic Center.\n\nReferences\n\n\nBloem-Reddy, Benjamin, and Yee Whye Teh. 2020. “Probabilistic Symmetries and Invariant Neural Networks.” J. Mach. Learn. Res. 21: 90–91.\n\n\nDeans, Matthew C. 2002. “Maximally Informative Statistics for Localization and Mapping.” In Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No. 02CH37292), 2:1824–29. IEEE.\n\n\nMüller, Jens, Robert Schmier, Lynton Ardizzone, Carsten Rother, and Ullrich Köthe. 2021. “Learning Robust Models Using the Principle of Independent Causal Mechanisms.” In DAGM German Conference on Pattern Recognition, 79–110. Springer.\n\n\nRadev, Stefan T, Ulf K Mertens, Andreas Voss, Lynton Ardizzone, and Ullrich Köthe. 2020. “BayesFlow: Learning Complex Stochastic Models with Invertible Neural Networks.” IEEE Transactions on Neural Networks and Learning Systems."
  },
  {
    "objectID": "BayesianKrig/abstract.html",
    "href": "BayesianKrig/abstract.html",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "",
    "text": "\\[\n    \\newcommand{\\pluseq}{\\mathrel{+}=}\n\\]"
  },
  {
    "objectID": "BayesianKrig/abstract.html#objectives-and-scope-100-words",
    "href": "BayesianKrig/abstract.html#objectives-and-scope-100-words",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "OBJECTIVES AND SCOPE (100 words)",
    "text": "OBJECTIVES AND SCOPE (100 words)\nThe objective is to demonstrate Normalizing Flows for subsurface kriging from wells. We will show that after supervised training of our method, we can generate multiple realistic samples of plausible earth models that match the observed wells. We observe that these samples produce uncertainty statistics that are correlated with the error made by the method. Finally, we compare the speed and quality of our solutions with those obtained using a traditional variogram approach."
  },
  {
    "objectID": "BayesianKrig/abstract.html#methods-procedures-process-250-words",
    "href": "BayesianKrig/abstract.html#methods-procedures-process-250-words",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "METHODS, PROCEDURES, PROCESS (250 words)",
    "text": "METHODS, PROCEDURES, PROCESS (250 words)\nKriging is highly ill-posed (there is no unique solution) so the preferred method should be able to produce many models that match the well logs. Generative deep learning can be used to sample models that are conditioned on observations. A particular class of generative deep learning are normalizing flows. These are particularly attractive because they are fast to sample from and their low training memory requirements. We implemented an architecture in Julia with InvertibleNetworks.jl.\nOur method is supervised so needs training examples of observed wells y where the corresponding earth models x. We use the compass model with a 90%/5%/5% training/validation/test split. For each training slice (nz=256, nx=512, d=10m) of the compass volume, we randomly generate well observations by selecting 5 columns at least 200 meters distance between each. This process creates the training pairs (x_i,y_i) used to train the conditional normalizing flow.\nAfter training, we input an unseen well log y and produce samples of the posterior p(x|y). To create single point estimate, we average all posterior samples to get the posterior mean.\nFor the variogram, we use exponential ordinary variogram from the package PyKrige. The variogram parameters are automatically selected by the well log data. We manually set the anisotropy angle to 0 to match the strongly horizontal Compass model."
  },
  {
    "objectID": "BayesianKrig/abstract.html#results-observations-conclusions-250-words",
    "href": "BayesianKrig/abstract.html#results-observations-conclusions-250-words",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "RESULTS, OBSERVATIONS, CONCLUSIONS (250 words)",
    "text": "RESULTS, OBSERVATIONS, CONCLUSIONS (250 words)\nIn Figure 2, we show posterior samples from our method where each sample takes 10ms to compute. To validate the quality of the earth models produced by our method, we compare the posterior mean with known ground truth models from a leave-out test set. We compare various metrics (SSIM PSNR RSME) and time-to-compute and verify that our method produces higher quality reconstructions while being faster than a variogram approach.\nWe also look at the uncertainty of our approach by looking at the intrasample variation between the posterior samples. We plot these and compare with the calculated standard variation of the variogram. Compared against the variograms variance our method produces uncertainty results that are more interpretable and correlate with errors made in specific structures.\nThe table in Figure 2 shows the quantitative performance of our method, in summary, our method takes less than 2 seconds to produce a high quality point estimates with average RMSE of 0.038 compared to the variogram with average RMSE of 0.043. The posterior mean will give less error on average, but the earth models are smoothed thus for downstream tasks, we recommend practitioners use posterior samples as they maintain realistic earth characteristics. We conclude that our method is a promising option for creating realistic earth models that match observed data wells and that it offers quantitative advantages over traditional approaches."
  },
  {
    "objectID": "BayesianKrig/abstract.html#significancenovelty-100-words",
    "href": "BayesianKrig/abstract.html#significancenovelty-100-words",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "SIGNIFICANCE/NOVELTY (100 words)",
    "text": "SIGNIFICANCE/NOVELTY (100 words)\nWe introduce the first use of conditional normalizing flows for kriging. While previous implementations of conditional normalizing flows have struggled on high dimensional images, we have demonstrated that our software implementation allows for learning distributions over large images. This method is set to scale to 3D image volumes in future work which will further enable the application of these methods to realistic seismic problems. In contrast with traditional variogram methods, our framework produces realistic samples, that is particularly important for downstream tasks in reservoir engineering and other applications where multiple plausible models are needed.\n\n\n\n\n\n\n\nfig1\n\n\n\n\nFigure 1: Schematic of full training process and test time evaluation of our method.\n\n\n\n\n\n\n\n\n\nfig2\n\n\n\n\nFigure 2: Results from our method compared with exponential variogram. Our method produces realistic samples of earth models that when averaged produce high quality point estimates."
  },
  {
    "objectID": "yin2023IMAGEend2end/abstract.html",
    "href": "yin2023IMAGEend2end/abstract.html",
    "title": "Coupled physics inversion for geological carbon storage monitoring",
    "section": "",
    "text": "Before a geological carbon storage project, reservoir engineers need permeability model(s) of the reservoir to predict CO2 dynamics in the Earth subsurface via fluid-flow simulation. However, the initial permeability is never known accurate enough beforehand, which can lead to wrong CO2 plume predictions. Through the coupled physics inversion framework, we show that time-lapse seismic data can be used along with the project to invert for the permeability model. This coupled physics inversion results in better CO2 plume recovery for the past vintages, and more accurate CO2 plume predictions in the future. The accurate predictions helps practitioners to estimate of the reservoir storage capacity along with the ongoing project, and prevent potential risks such as CO2 leakage through fault and fractures."
  },
  {
    "objectID": "yin2023IMAGEend2end/abstract.html#methods-procedures-process-250-words",
    "href": "yin2023IMAGEend2end/abstract.html#methods-procedures-process-250-words",
    "title": "Coupled physics inversion for geological carbon storage monitoring",
    "section": "Methods, Procedures, Process (250 words)",
    "text": "Methods, Procedures, Process (250 words)\nWe adopt the coupled physics inversion framework that involves three kinds of physics, namely fluid-flow, rock, and wave physics. The fluid-flow equations model the CO2 dynamics in the Earth subsurface according to permeability of the reservoir. The rock physics modeling maps the time-varying CO2 concentration in the rocks to the time-varying changes of acoustic wavespeed of the rocks. Finally, seismic modeling generates time-lapse seismic data for each vintage based on the wavespeed of the rocks. A schematic workflow is shown in Figure 1.\nGiven observed time-lapse seismic data, we invert the nested physics modeling operators for the permeability in an end-to-end fashion. The inverted permeability can be used to generate time-varying CO2 concentration snapshots that match observed time-lapse seismic data. Without further collected seismic data after the injection is terminated, practitioners can also run fluid-flow simulation on the inverted permeability with their own choices of injection rate/location to forecast the CO2 plume in the future and estimate the storage capacity of the reservoir."
  },
  {
    "objectID": "yin2023IMAGEend2end/abstract.html#results-observations-conclusions-250-words",
    "href": "yin2023IMAGEend2end/abstract.html#results-observations-conclusions-250-words",
    "title": "Coupled physics inversion for geological carbon storage monitoring",
    "section": "Results, Observations, Conclusions (250 words)",
    "text": "Results, Observations, Conclusions (250 words)\nWe conduct a near realistic numerical study on Compass model, which is currently considered as a potential site for geological carbon storage. We convert the compressional wavespeed in the model to log permeability values to make up alternating high and low permeability layers in the reservoir with a 50m depth seal on the top. We inject 1 million metric ton CO2 per year in a highly permeable layer for 25 years. CO2 tends to move into high permeability layers (over 1000 millidarcies) and move up due to buoyancy effects. We shoot 5 vintages of crosswell seismic data at year 5, 10, 15, 20, and 25, with a ricker wavelet with central frequency 20 Hz.\nWe start inversion with homogenous permeability values in the reservoir. After 12 datapass of gradient descent iterations with back-tracking line search, we recover the high permeability layers upon which CO2 plume has ever touched in these 25 years, shown in Figure 2 (a). We do acknowledge errors on the recovered permeability, especially at the locations upon which CO2 plume has not touched. The predicted snapshots of CO2 plume from the inverted permeability are shown in Figure 2 (b), which looks drastically different than the initial prediction on the homogeneous permeability, but reasonably similar to the ground truth ones.\nWe stop the injection at the 25th year and keep simulating the CO2 plume for another 75 years. The predicted CO2 plume from inverted permeability is again close to the ground truth ones with acceptable errors in especially near-future snapshots."
  },
  {
    "objectID": "yin2023IMAGEend2end/abstract.html#significancenovelty-100-words",
    "href": "yin2023IMAGEend2end/abstract.html#significancenovelty-100-words",
    "title": "Coupled physics inversion for geological carbon storage monitoring",
    "section": "Significance/Novelty (100 words)",
    "text": "Significance/Novelty (100 words)\nWe consider this as the first numerical case study that brings the coupled physics inversion framework to a realistic Earth model as a potential geological carbon storage site. Moreover, we convert wavespeed model, with real heterogeneity from well log and imaged seismic, to a near realistic permeability model. We conduct the fluid-flow simulation following a professional and standardized fluid-flow solver based on Jutul.jl, which correctly models near realistic two-phase compressible flows and the capillary pressure effects. We also show that the recovered permeability can be used to forecast the dynamics of the CO2 plume in the future without any seismic observation data.\n\n\n\n\n\n\n\nfig1\n\n\n\n\nFigure 1: Figure 1\n\n\n\n\n\n\n\n\n\nfig2\n\n\n\n\nFigure 2: Figure 2"
  },
  {
    "objectID": "BayesianFWI/abstract.html",
    "href": "BayesianFWI/abstract.html",
    "title": "Amortized Bayesian Full Waveform Inversion and Experimental Design with Normalizing Flows",
    "section": "",
    "text": "Probabistic approaches to Full Waveform Inversion (FWI), such as Bayesian ones, traditionally require expensive computations. To reduce this computational burden at test time, we propose to amortize the computational cost with an offline training phase. After training, our method aims to efficiently generate probabilistic FWI solutions with uncertainty information. This objective is achieved by exploiting the ability of deep generative neural networks (i.e. Normalizing Flows) to learn distributions, in this case, the Bayesian posterior. The uncertainty information in the posterior is used during offline training to find the optimal experimental design with respect to receiver placement.\n\n\n\nNormalizing flows (NFs) are generative networks that can learn to sample from conditional distributions, i.e. our desired Bayesian posterior p(x|y) where x are earth models are y is seismic data. To train NFs, we require training pairs of these earth models and seismic data. We use pairs from open source dataset OPENFWI. The earth models are 64x64 size and the seismic data is simulated with 5 source experiments with 15Hz freuquency and 15dB noise.\nThe NFs training objective (illustrated in Figure 1) has been shown to be equivalent to maximizing the information gain used in Bayesian experimental design. Thus we propose to jointly optimize a mask M that occludes receivers of the seismic data (M.*y) during NF training. Our optimization jointly optimizes for NF parameters and optimal mask M values in a single objective (Equation 1 in Figure 1).\nWe use the CurveFault_B dataset from OPENFWI which contains 55k pairs. We use a 90/5/5 split.\n\n\n\nAfter training, our method can generate posterior samples at the cost of one neural network pass (10ms on our GPU). Seen in Figure 2, the posterior samples are realistic earth models that could plausibly match the seismic data. The small variations between the models are due to the FWI problem being ill-posed because of its noisy indirect observation of the subsurface. To study these variations we take the sample.\nTo make a single high-quality solution, we take the mean of the posterior samples. The posterior mean shows high-quality metrics that surpass previous benchmarks on the OPENFWI dataset while our method produces Bayesian uncertainty and the benchmark methods do not.\nThe optimal experimental design found by our method (filled in circles in Figure 2 represent optimal receiver locations) matches prior expectations that the most important information is contained in short offsets. This result was independently discovered by the Bayesian optimization routine.\n\n\n\nTo our knowledge, this is the first demonstration of conditional normalizing flows on amortized FWI with uncertainty quantification. We also showed a practical application of the uncertainty towards experimental design. While the open source dataset has small models, NFs are memory efficient due to their intrinsic invertibility so are set to scale to realistic sized problems."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Image2023",
    "section": "",
    "text": "This is a Quarto website.\nAll submissions to the Image23 conference with additional figures, references, …\nList of abstracts:\n\nLearned one-shot imaging Learned non-linear simultenous source and corresponding supershot for seismic imaging.\nCoupled physics inversion Coupled physics inversion for geological carbon storage monitoring.\nBayesian FWI Amortized Bayesian Full Waveform Inversion and Experimental Design with Normalizing Flows\nBayesian Kriging Generative Seismic Kriging with Normalizing Flows"
  }
]