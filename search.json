[
  {
    "objectID": "OneShot/abstract.html",
    "href": "OneShot/abstract.html",
    "title": "Learned one-shot imaging",
    "section": "",
    "text": "\\[\n    \\newcommand{\\pluseq}{\\mathrel{+}=}\n\\]"
  },
  {
    "objectID": "OneShot/abstract.html#summary-networks-and-probabilistic-symmetry",
    "href": "OneShot/abstract.html#summary-networks-and-probabilistic-symmetry",
    "title": "Learned one-shot imaging",
    "section": "Summary networks and probabilistic symmetry",
    "text": "Summary networks and probabilistic symmetry\nrefs: (Deans 2002) -Summary statistics reduce the size of incoming datasets while maintainting the same posterior distribution p(x|y) = p(x|summary) (Radev et al. 2020) -Summary networks learn to reduce the size of incoming datasets and maximize informativeness of the summarized data due to joint learning of summary network and posterior learning network. -hand waves an argument that jointly trained networks will maximize the mutual information between h(y) and x (MÃ¼ller et al. 2021) -Goes in to further detail and rigoursly proves that jointly trained networks will maximize the mutual information between h(y) and x (Bloem-Reddy and Teh 2020) suggests to use learned layers that are invariant under a certain transformation. This transformation is described by the probabilistic assumption on your data.\nExample paragraph: This work takes inspiration from the concept of a summary network (Radev et al. 2020) these are networks that compress observables \\(d_{obs}\\) while maximining information useful for inference of un-observables \\(x\\). To guide the architectural design of a summary network (Bloem-Reddy and Teh 2020) suggests to use learned layers that respect the probabalistic symmetry of the data. Practically, this is accomplished by making the layers be invariant under a certain transformation. For the case active source seismic imaging, i.i.d sampling entails the assumption that the order of sources does not matter. This assumption is implicit in the sum structure of RTM/gradient calculations. Therefore it is invariant with respect to permutation transformation. Our approach is most similar to this case, since we use a UNet with many input channels that output a single channel. In our testing, having a separate network for each dataset did not perform better thatn simply inputing each dataset along a channel."
  },
  {
    "objectID": "OneShot/abstract.html#supervised",
    "href": "OneShot/abstract.html#supervised",
    "title": "Learned one-shot imaging",
    "section": "Supervised",
    "text": "Supervised\nTHe simplest formulation aims to learn the super-shot and simultenaous source that best inform the model perturbation, given the surfac recorded data. Mathematically, it means that we are trying to fit the true mode lperturbation with two networks that learn a single super-shot and simultenous source for the Jacobian from the indicudual field recorded shot records. Mathematically, the learning can be written as: \\[\nmin_{\\theta, \\phi} \\ \\mathbb{E}\\left[ J(\\mathcal{H}_{\\phi}(\\mathcal{G}_{\\theta}(d_{\\text{obs}}))^\\top \\mathcal{G}_{\\theta}(d_{\\text{obs}}) - \\delta m \\right]\n\\tag{1}\\]\nwhere \\(\\mathcal{H}_{\\phi}, \\mathcal{G}_{\\theta}\\) are the two networks mapping the individual shot records into a single super-shot (the learned simultenous-source is learned at the receiver locations), \\(J\\) is the conventionnal adjoint Born imaging operator, \\(d_{\\text{obs}}\\) is the observed data and \\(\\delta m\\) is the model perturbation. We note that to compute an update on the two networks simultenaously, the gradient of the Jacobian with respect to its source is necessary. This derivative is however trivial to obtain with JUDI.jl thanks to its high-level linear algebra abstraction and integration with automatic differentiation framework in Julia."
  },
  {
    "objectID": "OneShot/abstract.html#unsupervised",
    "href": "OneShot/abstract.html#unsupervised",
    "title": "Learned one-shot imaging",
    "section": "Unsupervised",
    "text": "Unsupervised\n\\[\nmin_{\\theta, \\phi} \\ \\mathbb{E}\\left[ \\tilde{J}_{\\text{rtm}} J(\\mathcal{H}_{\\phi}(\\mathcal{G}_{\\theta}(d_{\\text{obs}}))^\\top \\mathcal{G}_{\\theta}(d_{\\text{obs}}) - \\tilde{d}_{\\text{obs}} \\right ]\n\\tag{2}\\]\nWhere \\(\\tilde{d}_{\\text{obs}} = \\sum_{i=1}^{n_src} w_i d_{\\text{obs}},i\\) is a random super shot with \\(w_i \\mathcal{N}(0, 1)\\) and \\(\\tilde{J}\\) is the corresponding simultenous source born modeling operator. While this formulation ivolves an additional demigration (and therfore and additional migration to compute hte gradient), we do not require any knowledge of the true model perturbation but only the data. We could therefore in theory use this formulation for a wide range of datasets at once to generalize to any survey."
  },
  {
    "objectID": "OneShot/abstract.html#code-availability",
    "href": "OneShot/abstract.html#code-availability",
    "title": "Learned one-shot imaging",
    "section": "Code availability",
    "text": "Code availability\nTBD"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Image2023",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]