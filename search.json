[
  {
    "objectID": "zhang2023IMAGEsg/abstract.html",
    "href": "zhang2023IMAGEsg/abstract.html",
    "title": "3D seismic survey design by maximizing the spectral gap",
    "section": "",
    "text": "The huge cost of 3D seismic acquisition calls for methods to reduce the number of receivers in the acquisition by designing optimal receiver sampling masks. Recent studies on 2D seismic show that maximizing the spectral gap of the subsampling mask leads to better wavefield reconstruction results according to expander graph theory. We enrich the current study by proposing a simulation-free method to automatically generate optimal 3D seismic acquisition by maximizing the spectral gap of the subsampling mask via simulated annealing algorithm. Numerical experiments confirm the success of the proposed acquisition design method over jittered sampling scheme."
  },
  {
    "objectID": "zhang2023IMAGEsg/abstract.html#methods-procedures-process-250-words",
    "href": "zhang2023IMAGEsg/abstract.html#methods-procedures-process-250-words",
    "title": "3D seismic survey design by maximizing the spectral gap",
    "section": "Methods, Procedures, Process (250 words)",
    "text": "Methods, Procedures, Process (250 words)\nThe spectral gap ratio is the ratio of the first and second singular values of a binary subsampling mask. It is a cheap-to-compute measure to predict wavefield reconstruction quality based on the binary mask. Motivated by recent success on 2D seismic survey design methods driven by spectral gap ratio minimization, we consider the 3D seismic survey design where sources are missing and receivers are fully sampled. We propose a 3D seismic survey design method via minimizing the spectral gap ratio of 3D source sampling mask.\nBecause 3D wavefield reconstruction based on low-rank matrix completion is shown to be the best performant in the non-canonical organization domain, we aim to minimize the spectral gap ratio of subsampling mask in this domain. Fortunately, when receivers are fully sampled, each subsampled source location becomes a fully sampled square in this mask. As a result, the spectral gap ratio in the non-canonical domain is exactly the same as the spectral gap ratio in the common receiver domain (see Figure 1). Therefore, we implement the simulated annealing algorithm to iteratively find source subsampling masks that minimize the spectral gap ratio in the common receiver domain. The main computational cost of this algorithm is computing the first two singular values of the source subsampling mask, which is negligible compared to approaches that require wave simulations. The resulting optimal mask with the lowest spectral gap ratio indicates the source sampling locations that favor 3D wavefield reconstruction via matrix completion in non-cononical organization domain."
  },
  {
    "objectID": "zhang2023IMAGEsg/abstract.html#results-observations-conclusions-250-words",
    "href": "zhang2023IMAGEsg/abstract.html#results-observations-conclusions-250-words",
    "title": "3D seismic survey design by maximizing the spectral gap",
    "section": "Results, Observations, Conclusions (250 words)",
    "text": "Results, Observations, Conclusions (250 words)\nTo illustrate the efficacy of our method via a numerical experiment on a simulated 3D marine dataset over the compass model. The data volume consists of \\(501 \\times 100 \\times 100 \\times 41 \\times 41\\) entries—i.e., \\(n_t \\times n_{rx} \\times n_{ry} \\times n_{sx} \\times n_{sy}\\) along the time, receiver \\(x\\), receiver \\(y\\), source \\(x\\), and source \\(y\\) directions. The distance between the adjacent sources and receivers are 150m and 25m, respectively, with time interval 0.01s. By removing 90% of receivers using jittered subsampling, we obtain a binary matrix with the spectral gap ratio 0.507 in the non-canonical domain. After applying simulated annealing algorithm with 80000 iterations, the spectral gap ratio of mask effectively decreases to 0.328. To validate the efficacy of our acquisition design method, we perform data reconstruction on a 16.8Hz frequency slice via weighted matrix completion for the two subsampled datasets with jittered subsampling mask and the proposed mask, with results shown in Figure 2. The reconstruction signal-to-noise ratio from the observed data at proposed source locations is 12.27 dB, which is about 1.4 dB higher than the reconstruction signal-to-noise ratio from the observed data at jittered sampled source locations as 10.88 dB. This confirms that the proposed optimized sources sampling locations result in a superior seismic survey that leads to better wavefield reconstruction performance."
  },
  {
    "objectID": "zhang2023IMAGEsg/abstract.html#significancenovelty-100-words",
    "href": "zhang2023IMAGEsg/abstract.html#significancenovelty-100-words",
    "title": "3D seismic survey design by maximizing the spectral gap",
    "section": "Significance/Novelty (100 words)",
    "text": "Significance/Novelty (100 words)\nThis is the first numerical case study that applies spectral gap ratio minimization techniques for seismic acquisition design to 3D wavefield reconstruction. Rather than requiring costly wave simulations, the proposed method for optimizing binary masks is computationally inexpensive. Experiments demonstrate that the proposed method generates an improved 3D seismic survey suitable for 3D wavefield reconstruction.\n\n\n\n\n\n\n\nfig1\n\n\n\n\nFigure 1: Figure 1\n\n\n\n\n\n\n\n\n\nfig2\n\n\n\n\nFigure 2: Figure 2"
  },
  {
    "objectID": "OneShot/abstract.html",
    "href": "OneShot/abstract.html",
    "title": "Learned one-shot imaging",
    "section": "",
    "text": "\\[\n    \\newcommand{\\pluseq}{\\mathrel{+}=}\n\\]"
  },
  {
    "objectID": "OneShot/abstract.html#objectives-and-scope-100-words",
    "href": "OneShot/abstract.html#objectives-and-scope-100-words",
    "title": "Learned one-shot imaging",
    "section": "OBJECTIVES AND SCOPE (100 words)",
    "text": "OBJECTIVES AND SCOPE (100 words)\nSeismic imaging’s main limiting factor is the scale of the involved dataset and the number of independent wave-equations solves required to migrate thousands of shots. To tackle this dimensionality curse, we introduce a learned framework that extends the conventional computationally reductive linear source superpositions (simultaneous sources) to a non-linear learned superposition and its corresponding super-shot. With this method, we can obtain an image of the subsurface at the cost of a single shot migration by learning the most informative superposition of shots."
  },
  {
    "objectID": "OneShot/abstract.html#methods-procedures-process-250-words",
    "href": "OneShot/abstract.html#methods-procedures-process-250-words",
    "title": "Learned one-shot imaging",
    "section": "METHODS, PROCEDURES, PROCESS (250 words)",
    "text": "METHODS, PROCEDURES, PROCESS (250 words)\nSimultenous source imaging takes advantage of the linearity of the wave equation to add different shot records together reducing the cost of imaging. Because of the linearity, the same linear transformation can be applied both to the source and data to keep the problem well posed. Here, we introduce a non-linear superposition, and to compensate for the inability to apply the corrsponding transform to the source, we simultenously learn both the non-linear source and non-linear data with a neural network. This networks takes the shot records as input and outputs the learned supershot and simultenous source. The network is trained in two ways. First we train a supervised network minimizing the difference between the migrated non-linear supershot and the true model perturbation which requires access to the true model. Since in practice the true subsurface image is unknown, we train a second unsupervised network minimizing the difference between the true data and the migrated-demingrated supershot."
  },
  {
    "objectID": "OneShot/abstract.html#results-observations-conclusions-250-words",
    "href": "OneShot/abstract.html#results-observations-conclusions-250-words",
    "title": "Learned one-shot imaging",
    "section": "RESULTS, OBSERVATIONS, CONCLUSIONS (250 words)",
    "text": "RESULTS, OBSERVATIONS, CONCLUSIONS (250 words)\nWe show on Figure 1 the results with the supervised training and on Figure 2 with unsupervised trainging. We shot the learned super-shot and simultenous source next to the true observed data and compare the obtained one-shot subsurface image with standard multi-source RTM and conventional random simultenous sources imaging. We see that the image is not only less noisy that the standard simultenous source image but also a better representation of the subsurface than the conventional RTM. This demonstrate that we can obtain a good image of the earth at the cost of a single source migration while avoiding the artifacts associated with conventional simultenous source imaging."
  },
  {
    "objectID": "OneShot/abstract.html#significancenovelty-100-words",
    "href": "OneShot/abstract.html#significancenovelty-100-words",
    "title": "Learned one-shot imaging",
    "section": "SIGNIFICANCE/NOVELTY (100 words)",
    "text": "SIGNIFICANCE/NOVELTY (100 words)\nWe introduced the first instance of non-linear simultenaous source imaging. Because only linear transforms can be applied to the source and data using the linearity of the wave-equation, we introduced a neural network capable of creating a pair of simultenous source and super-shot that is most informative for migration. This learned on-shot imaging framework allows to drastically reduce the cost of seismic imaging while requiring limited training resources since only single-shot are migrated."
  },
  {
    "objectID": "OneShot/abstract.html#introduction",
    "href": "OneShot/abstract.html#introduction",
    "title": "Learned one-shot imaging",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "OneShot/abstract.html#methodology",
    "href": "OneShot/abstract.html#methodology",
    "title": "Learned one-shot imaging",
    "section": "Methodology",
    "text": "Methodology\nWe now introduce the formulation of aour learned simultenous source-data pair for seismic imaging. We derive two training problem where the first onbe rely on the knowledge of the true perturbation, while the second one solely rely on the observed data. Fundamentally, we are introducing a formulation that learns the most informative single super-shot and correspoinding source given either a subsurface refelctivity model or the surface recorded data.\n\nSummary networks and probabilistic symmetry\n(Deans 2002) -Summary statistics reduce the size of incoming datasets while maintainting the same posterior distribution p(x|y) = p(x|summary) (Radev et al. 2020) -Summary networks learn to reduce the size of incoming datasets and maximize informativeness of the summarized data due to joint learning of summary network and posterior learning network. -hand waves an argument that jointly trained networks will maximize the mutual information between h(y) and x (Müller et al. 2021) -Goes in to further detail and rigoursly proves that jointly trained networks will maximize the mutual information between h(y) and x (Bloem-Reddy and Teh 2020) suggests to use learned layers that are invariant under a certain transformation. This transformation is described by the probabilistic assumption on your data.\nExample paragraph: This work takes inspiration from the concept of a summary network (Radev et al. 2020) these are networks that compress observables \\(d_{obs}\\) while maximining information useful for inference of un-observables \\(x\\). To guide the architectural design of a summary network (Bloem-Reddy and Teh 2020) suggests to use learned layers that respect the probabalistic symmetry of the data. Practically, this is accomplished by making the layers be invariant under a certain transformation. For the case active source seismic imaging, i.i.d sampling entails the assumption that the order of sources does not matter. This assumption is implicit in the sum structure of RTM/gradient calculations. Therefore it is invariant with respect to permutation transformation. Our approach is most similar to this case, since we use a UNet with many input channels that output a single channel. In our testing, having a separate network for each dataset did not perform better thatn simply inputing each dataset along a channel.\n\n\nSupervised\nTHe simplest formulation aims to learn the super-shot and simultenaous source that best inform the model perturbation, given the surfac recorded data. Mathematically, it means that we are trying to fit the true mode lperturbation with two networks that learn a single super-shot and simultenous source for the Jacobian from the indicudual field recorded shot records. Mathematically, the learning can be written as: \\[\n\\min_{\\theta, \\phi} \\ \\mathbb{E}\\left[ J(\\mathcal{H}_{\\phi}(\\mathcal{G}_{\\theta}(d_{\\text{obs}}))^\\top \\mathcal{G}_{\\theta}(d_{\\text{obs}}) - \\delta m \\right]\n\\tag{1}\\]\nwhere \\(\\mathcal{H}_{\\phi}, \\mathcal{G}_{\\theta}\\) are the two networks mapping the individual shot records into a single super-shot (the learned simultenous-source is learned at the receiver locations), \\(J\\) is the conventionnal adjoint Born imaging operator, \\(d_{\\text{obs}}\\) is the observed data and \\(\\delta m\\) is the model perturbation. We note that to compute an update on the two networks simultenaously, the gradient of the Jacobian with respect to its source is necessary. This derivative is however trivial to obtain with JUDI.jl thanks to its high-level linear algebra abstraction and integration with automatic differentiation framework in Julia.\n\n\nUnsupervised\n\\[\n\\min_{\\theta, \\phi} \\ \\mathbb{E}\\left[ \\tilde{J}_{\\text{rtm}} J(\\mathcal{H}_{\\phi}(\\mathcal{G}_{\\theta}(d_{\\text{obs}}))^\\top \\mathcal{G}_{\\theta}(d_{\\text{obs}}) - \\tilde{d}_{\\text{obs}} \\right ]\n\\tag{2}\\]\nWhere \\(\\tilde{d}_{\\text{obs}} = \\sum_{i=1}^{n_src} w_i d_{\\text{obs},i}\\) is a random super shot with \\(w_i := \\mathcal{N}(0, 1)\\) and \\(\\tilde{J}\\) is the corresponding simultenous source born modeling operator. While this formulation ivolves an additional demigration (and therfore and additional migration to compute hte gradient), we do not require any knowledge of the true model perturbation but only the data. We could therefore in theory use this formulation for a wide range of datasets at once to generalize to any survey."
  },
  {
    "objectID": "OneShot/abstract.html#synthetic-case-studies",
    "href": "OneShot/abstract.html#synthetic-case-studies",
    "title": "Learned one-shot imaging",
    "section": "Synthetic case studies",
    "text": "Synthetic case studies\nWe illustrate our method on a realstic 2D imaging problem. We created a dataset of 2000 2D slices by extracting slices out of the 3D overthrust model. We then split this dataset into 1600 slices for trainng and 400 slices for testing. For each 2D slice, we generate 21 shot records. One of the main advantage of our one-shot imaging method is that we only require a single migration-demigration per iteration. Therefore, we can perform 21 epochs before arriving to a computationnal cost equivalent to the plain standard RTM imaging of each slice. Since we only perform 15 epochs, our method is overall cheapper than computing the RTM on every single shot if we include the cost of training.\nWe trained the networks, both in the supervised and unsupervised case, for 15 epochs with a learning rate of \\(.0004\\) using the Adam optimizer.\n\n\n\n\n\n\n\nData\n\n\n\n\n\n\n\nRTMs\n\n\n\n\nFigure 1: Learned super-shot and simultenous sources on a testing slice. We show a shot record and the moirated image with a random supershot and with sequential shots for reference.\n\n\n\n\n\n\n\n\n\nData\n\n\n\n\n\n\n\nRTMs\n\n\n\n\nFigure 2: Learned super-shot and simultenous sources on a testing slice. We show a shot record and the moirated image with a random supershot and with sequential shots for reference.\n\n\n\nCode availability\nResults presented here can be reproduced with the software and examples in OneShotImaging.jl."
  },
  {
    "objectID": "OneShot/abstract.html#discussion-and-conclusions",
    "href": "OneShot/abstract.html#discussion-and-conclusions",
    "title": "Learned one-shot imaging",
    "section": "Discussion and conclusions",
    "text": "Discussion and conclusions\nWe introduced data-domain learning method that provides high accuracy imagies of the subsurface through one-shot imaging. We trained a network that learns the simultenous source and super-shot that most inform the subsurface from the field recorded data. We showed that we obtain high accuracy images of the subsurface that contain broader frequency range than standard imaging and does not require prior knowledge of the source. Additionnally, the overall computationnal cost of training does not exceed the traditionnal cost of imaging."
  },
  {
    "objectID": "OneShot/abstract.html#acknowledgement",
    "href": "OneShot/abstract.html#acknowledgement",
    "title": "Learned one-shot imaging",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThis research was carried out with the support of Georgia Research Alliance and partners of the ML4Seismic Center.\n\nReferences\n\n\nBloem-Reddy, Benjamin, and Yee Whye Teh. 2020. “Probabilistic Symmetries and Invariant Neural Networks.” J. Mach. Learn. Res. 21: 90–91.\n\n\nDeans, Matthew C. 2002. “Maximally Informative Statistics for Localization and Mapping.” In Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No. 02CH37292), 2:1824–29. IEEE.\n\n\nMüller, Jens, Robert Schmier, Lynton Ardizzone, Carsten Rother, and Ullrich Köthe. 2021. “Learning Robust Models Using the Principle of Independent Causal Mechanisms.” In DAGM German Conference on Pattern Recognition, 79–110. Springer.\n\n\nRadev, Stefan T, Ulf K Mertens, Andreas Voss, Lynton Ardizzone, and Ullrich Köthe. 2020. “BayesFlow: Learning Complex Stochastic Models with Invertible Neural Networks.” IEEE Transactions on Neural Networks and Learning Systems."
  },
  {
    "objectID": "BayesianKrig/abstract.html",
    "href": "BayesianKrig/abstract.html",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "",
    "text": "\\[\n    \\newcommand{\\pluseq}{\\mathrel{+}=}\n\\]"
  },
  {
    "objectID": "BayesianKrig/abstract.html#objectives-and-scope-100-words",
    "href": "BayesianKrig/abstract.html#objectives-and-scope-100-words",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "OBJECTIVES AND SCOPE (100 words)",
    "text": "OBJECTIVES AND SCOPE (100 words)\nThe objective is to demonstrate Normalizing Flows for subsurface kriging from wells. We will show that after supervised training of our method, we can generate multiple realistic samples of plausible earth models that match the observed wells. We observe that these samples produce uncertainty statistics that are correlated with the error made by the method. Finally, we compare the speed and quality of our solutions with those obtained using a traditional variogram approach."
  },
  {
    "objectID": "BayesianKrig/abstract.html#methods-procedures-process-250-words",
    "href": "BayesianKrig/abstract.html#methods-procedures-process-250-words",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "METHODS, PROCEDURES, PROCESS (250 words)",
    "text": "METHODS, PROCEDURES, PROCESS (250 words)\nKriging is highly ill-posed (there is no unique solution) so the preferred method should be able to produce many models that match the well logs. Generative deep learning can be used to sample models that are conditioned on observations. A particular class of generative deep learning are normalizing flows. These are particularly attractive because they are fast to sample from and their low training memory requirements. We implemented an architecture in Julia with InvertibleNetworks.jl.\nOur method is supervised so needs training examples of observed wells y where the corresponding earth models x. We use the compass model with a 90%/5%/5% training/validation/test split. For each training slice (nz=256, nx=512, d=10m) of the compass volume, we randomly generate well observations by selecting 5 columns at least 200 meters distance between each. This process creates the training pairs (x_i,y_i) used to train the conditional normalizing flow.\nAfter training, we input an unseen well log y and produce samples of the posterior p(x|y). To create single point estimate, we average all posterior samples to get the posterior mean.\nFor the variogram, we use exponential ordinary variogram from the package PyKrige. The variogram parameters are automatically selected by the well log data. We manually set the anisotropy angle to 0 to match the strongly horizontal Compass model."
  },
  {
    "objectID": "BayesianKrig/abstract.html#results-observations-conclusions-250-words",
    "href": "BayesianKrig/abstract.html#results-observations-conclusions-250-words",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "RESULTS, OBSERVATIONS, CONCLUSIONS (250 words)",
    "text": "RESULTS, OBSERVATIONS, CONCLUSIONS (250 words)\nIn Figure 2, we show posterior samples from our method where each sample takes 10ms to compute. To validate the quality of the earth models produced by our method, we compare the posterior mean with known ground truth models from a leave-out test set. We compare various metrics (SSIM PSNR RSME) and time-to-compute and verify that our method produces higher quality reconstructions while being faster than a variogram approach.\nWe also look at the uncertainty of our approach by looking at the intrasample variation between the posterior samples. We plot these and compare with the calculated standard variation of the variogram. Compared against the variograms variance our method produces uncertainty results that are more interpretable and correlate with errors made in specific structures.\nThe table in Figure 2 shows the quantitative performance of our method, in summary, our method takes less than 2 seconds to produce a high quality point estimates with average RMSE of 0.038 compared to the variogram with average RMSE of 0.043. The posterior mean will give less error on average, but the earth models are smoothed thus for downstream tasks, we recommend practitioners use posterior samples as they maintain realistic earth characteristics. We conclude that our method is a promising option for creating realistic earth models that match observed data wells and that it offers quantitative advantages over traditional approaches."
  },
  {
    "objectID": "BayesianKrig/abstract.html#significancenovelty-100-words",
    "href": "BayesianKrig/abstract.html#significancenovelty-100-words",
    "title": "Generative Seismic Kriging with Normalizing Flows",
    "section": "SIGNIFICANCE/NOVELTY (100 words)",
    "text": "SIGNIFICANCE/NOVELTY (100 words)\nWe introduce the first use of conditional normalizing flows for kriging. While previous implementations of conditional normalizing flows have struggled on high dimensional images, we have demonstrated that our software implementation allows for learning distributions over large images. This method is set to scale to 3D image volumes in future work which will further enable the application of these methods to realistic seismic problems. In contrast with traditional variogram methods, our framework produces realistic samples, that is particularly important for downstream tasks in reservoir engineering and other applications where multiple plausible models are needed.\n\n\n\n\n\n\n\nfig1\n\n\n\n\nFigure 1: Schematic of full training process and test time evaluation of our method.\n\n\n\n\n\n\n\n\n\nfig2\n\n\n\n\nFigure 2: Results from our method compared with exponential variogram. Our method produces realistic samples of earth models that when averaged produce high quality point estimates."
  },
  {
    "objectID": "yin2023IMAGEend2end/abstract.html",
    "href": "yin2023IMAGEend2end/abstract.html",
    "title": "Coupled physics inversion for geological carbon storage monitoring",
    "section": "",
    "text": "Before a geological carbon storage project, reservoir engineers need permeability model(s) to predict CO2 dynamics in the Earth subsurface via fluid-flow simulation. However, the permeability model is never known accurate enough beforehand, which can lead to wrong CO2 plume predictions. Through the coupled physics inversion framework, we show that time-lapse seismic data can be used to invert for the permeability model. This results in better CO2 plume recovery for the past vintages, and more accurate predictions for future snapshots. The accurate predictions help practitioners to estimate storage capacity of the reservoir, and prevent risks such as CO2 leakage through fractures."
  },
  {
    "objectID": "yin2023IMAGEend2end/abstract.html#methods-procedures-process-250-words",
    "href": "yin2023IMAGEend2end/abstract.html#methods-procedures-process-250-words",
    "title": "Coupled physics inversion for geological carbon storage monitoring",
    "section": "Methods, Procedures, Process (250 words)",
    "text": "Methods, Procedures, Process (250 words)\nWe adopt the coupled physics inversion framework that involves three kinds of physics, namely fluid-flow, rock, and wave physics. The fluid-flow equations model the CO2 plume evolution in the Earth subsurface according to permeability of the reservoir. The rock physics modeling maps the time-varying CO2 concentration in the rocks to the time-varying changes of acoustic wavespeed of the rocks. Finally, seismic modeling generates time-lapse seismic data for each vintage based on the wavespeed of the rocks. A schematic workflow is shown in Figure 1.\nGiven observed time-lapse seismic data, we invert the nested physics modeling operators for the permeability in an end-to-end fashion. The inverted permeability can be used to generate time-varying CO2 concentration snapshots that match observed time-lapse seismic data. Without further collected seismic data after the injection is terminated, practitioners can also run fluid-flow simulation on the inverted permeability with their own choices of injection rate/location to forecast the CO2 plume in the future and estimate the storage capacity of the reservoir."
  },
  {
    "objectID": "yin2023IMAGEend2end/abstract.html#results-observations-conclusions-250-words",
    "href": "yin2023IMAGEend2end/abstract.html#results-observations-conclusions-250-words",
    "title": "Coupled physics inversion for geological carbon storage monitoring",
    "section": "Results, Observations, Conclusions (250 words)",
    "text": "Results, Observations, Conclusions (250 words)\nWe conduct a near realistic numerical study on Compass model, which is currently considered as a potential site for geological carbon storage. We convert the compressional wavespeed in the model to log permeability values to make up alternating high and low permeability layers in the reservoir with a seal on the top. We inject 1 million metric ton CO2 per year in a highly permeable layer for 25 years. CO2 tends to move into high permeability layers (over 1000 millidarcies) and move up due to buoyancy effects. We shoot 5 vintages of crosswell seismic data at every 5th year, with a ricker wavelet with central frequency 20 Hz.\nWe start inversion with homogenous permeability values in the reservoir. After 12 datapass of gradient descent iterations with back-tracking line search, the CO2 plume recovery from the inverted permeability is shown in Figure 2 (a). The extent of the plume looks drastically different than the initial prediction on the homogeneous permeability, but reasonably similar to the ground truth ones with acceptable errors. We recover the high and low permeability layers upon which CO2 plume has ever touched in these 25 years, shown in Figure 2 (b). We do acknowledge errors on the recovered permeability, especially at the locations upon which CO2 plume has not touched.\nWe stop the injection at the 25th year and keep simulating the CO2 plume for another 25 years. The CO2 plume forecast from inverted permeability is again close to the ground truth ones with acceptable errors in especially near-future snapshots."
  },
  {
    "objectID": "yin2023IMAGEend2end/abstract.html#significancenovelty-100-words",
    "href": "yin2023IMAGEend2end/abstract.html#significancenovelty-100-words",
    "title": "Coupled physics inversion for geological carbon storage monitoring",
    "section": "Significance/Novelty (100 words)",
    "text": "Significance/Novelty (100 words)\nWe consider this as the first numerical case study that brings the coupled physics inversion framework to a realistic Earth model as a potential geological carbon storage site. We convert the wavespeed model based on well logs and imaged seismic to a finely discretized permeability model with realistic heterogenous layers. We conduct the fluid-flow simulation following a professional solver based on JutulDarcy.jl, which correctly models near realistic two-phase compressible flows with capillary pressure effects. This near-realistic synthetic study confirms that coupled physics inversion can be used in geological carbon storage projects to estimate and forecast CO2 plume evolution.\n\n\n\n\n\n\n\nfig1\n\n\n\n\nFigure 1: Figure 1\n\n\n\n\n\n\n\n\n\nfig2\n\n\n\n\nFigure 2: Figure 2"
  },
  {
    "objectID": "BayesianFWI/abstract.html",
    "href": "BayesianFWI/abstract.html",
    "title": "Amortized Bayesian Full Waveform Inversion and Experimental Design with Normalizing Flows",
    "section": "",
    "text": "Probabistic approaches to Full-Waveform Inversion (FWI), such as Bayesian ones, traditionally require expensive computations that involve multiple wave-equation solves per iteration. To reduce this computational burden at test time, we propose to amortize the computational cost with an offline training. After training, our method aims to efficiently generate probabilistic FWI solutions with uncertainty information. This objective is achieved by exploiting the ability of deep generative neural networks (i.e. Normalizing Flows) to learn (conditional) distributions, in this case, the Bayesian posterior distribution. The uncertainty information in the posterior is used during offline training to find the optimal experimental design with respect to receiver placement.\n\n\n\nNormalizing flows (NFs) are generative networks that can learn to sample from conditional distributions, i.e. our desired Bayesian posterior p(x|y) where x are earth models are y is seismic data. To train NFs, we require training pairs of these earth models and seismic data. We use pairs from the open source dataset OPENFWI. The earth models are 64x64 size and the seismic data is simulated with 5 source experiments with 15Hz frequency and 15dB noise.\nThe NFs training objective (illustrated in Figure 1) has been shown to be equivalent to maximizing the information gain used in Bayesian experimental design. Thus, we propose to jointly optimize a mask M that occludes receivers of the seismic data (M.*y) during NF training. Our optimization jointly optimizes for NF parameters and optimal mask M values in a single objective (Equation 1 in Figure 1).\nWe use the CurveFault_B dataset from OPENFWI that contains 55k pairs. We use a 90/5/5 split for training, validation, and testing.\n\n\n\nAfter training, our method generates posterior samples at the cost of one neural network pass (10ms on our GPU). Seen in Figure 2, the posterior samples are realistic earth models that could plausibly match the seismic data. The small variations between the models are due to the FWI problem being ill-posed and because of noise in the indirect observations at the subsurface. To study these variations we take the sample. \nTo make a single high-quality solution, we take the mean of the posterior samples. The posterior mean shows high-quality metrics that surpass previous benchmarks on the OPENFWI dataset while our method also produces Bayesian uncertainty and the benchmark methods do not.\nThe optimal experimental design found by our method (filled in circles in Figure 2 represent optimal receiver locations) matches prior expectations that the most important information is contained in the short offsets. This result was independently discovered by the Bayesian optimization routine.\n\n\n\nTo our knowledge, this is the first demonstration of conditional normalizing flows on amortized FWI with uncertainty quantification. We also showed a practical application of the uncertainty towards experimental design. While the open source dataset has small models, NFs are memory efficient due to their intrinsic invertibility so are set to scale to realistic sized problems."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Image2023",
    "section": "",
    "text": "This is a Quarto website.\nAll submissions to the Image23 conference with additional figures, references, …\nList of abstracts:\n\nLearned one-shot imaging Learned non-linear simultenous source and corresponding supershot for seismic imaging.\nCoupled physics inversion Coupled physics inversion for geological carbon storage monitoring.\nBayesian FWI Amortized Bayesian Full Waveform Inversion and Experimental Design with Normalizing Flows\nBayesian Kriging Generative Seismic Kriging with Normalizing Flows\n3D survey design 3D seismic survey design by maximizing the spectral gap"
  }
]